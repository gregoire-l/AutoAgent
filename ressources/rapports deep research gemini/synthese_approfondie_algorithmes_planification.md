# **Synthèse de l'État de l'Art Algorithmique pour la Planification Agentique Avancée**

## **1\. Introduction : Brève présentation des défis conceptuels du modèle PI-QPM**

Le modèle "Pragmatic & Iterative Quantum Project Management" (PI-QPM) propose une approche novatrice pour la gestion de projets complexes, orchestrée par des agents logiciels. Sa philosophie repose sur une exploration ciblée d'un "Graphe de Potentiel Limité", un graphe de connaissances (KG) représentant un ensemble restreint des chemins d'exécution les plus prometteurs, incluant faits, hypothèses et probabilités. La planification s'effectue via une "Exploration Itérative à Gain Décroissant", où la boucle de réflexion s'interrompt lorsque l'apport informationnel d'une exploration supplémentaire devient marginal. La "bonté" d'un plan est évaluée par un "Jugement Objectif" s'appuyant sur des algorithmes non-subjectifs basés sur des métriques quantifiables (coût, risque, valeur) et des techniques d'optimisation multi-objectifs, notamment la recherche du Front de Pareto. Plutôt que de suivre un plan linéaire, le système privilégie une "Exécution par Point de Levier", sélectionnant l'action qui maximise le gain d'information ou fait progresser le plus significativement les stratégies prometteuses. Enfin, une "Mise à Jour Probabiliste" continue des croyances du système sur les chemins potentiels est effectuée en fonction des résultats des actions.

Ces caractéristiques fondamentales du modèle PI-QPM soulèvent des défis conceptuels majeurs qui nécessitent le recours à des formalismes et algorithmes de pointe en intelligence artificielle. Les cinq composantes du PI-QPM ne sont pas des éléments isolés ; elles dessinent un système où la perception (incarnée par le KG probabiliste), la délibération (comprenant la planification itérative et l'optimisation multi-objectifs), et l'action (manifestée par l'exécution ciblée et la mise à jour des croyances) sont intrinsèquement liées. Ces éléments s'influencent mutuellement et opèrent sous un régime d'incertitude omniprésent. Le "Graphe de Potentiel Limité", par exemple, est dynamiquement altéré par la "Mise à Jour Probabiliste", elle-même alimentée par les "Résultats des actions" qui découlent de "l'Exécution par Point de Levier". Cette exécution est, à son tour, guidée par une "Exploration Itérative" dont la continuation est conditionnée par un critère de "Gain Décroissant", et où les plans envisagés sont scrutés par un "Jugement Objectif". Ce cycle de perception-décision-action continu et adaptatif constitue le cœur du PI-QPM. Par conséquent, le défi principal ne réside pas uniquement dans l'identification d'algorithmes état de l'art pour chaque composant individuel, mais plutôt dans la sélection d'approches qui, par leur nature conceptuelle, favorisent une intégration cohérente et synergique. La compatibilité et l'interopérabilité théorique des algorithmes choisis pour chaque pilier seront donc d'une importance cruciale. Par exemple, un formalisme de graphe de connaissances probabiliste (abordé dans le Pilier 4\) doit pouvoir être exploité efficacement par des planificateurs opérant sous incertitude (Pilier 1\) et par des stratégies d'exploration fondées sur le gain d'information (Pilier 2). Ce rapport vise à synthétiser l'état de l'art académique pertinent pour adresser ces défis, en se concentrant sur les avancées théoriques récentes, particulièrement celles publiées après 2022\.

## **2\. Analyse Approfondie des Piliers Conceptuels du PI-QPM**

### **Pilier 1 : Planification et Décomposition sous Incertitude**

#### **2.1.a. Introduction au problème théorique du pilier**

Ce pilier aborde la nécessité de planifier et de décomposer des tâches complexes dans des environnements où l'information est partielle et les issues des actions stochastiques. Pour le modèle PI-QPM, cela se traduit par la capacité des agents à naviguer au sein du "Graphe de Potentiel Limité" en générant et évaluant des séquences d'actions (plans partiels ou complets) dont les conséquences ne sont pas certaines. La décomposition hiérarchique des tâches est un mécanisme essentiel pour gérer la complexité inhérente aux projets d'envergure, permettant de raffiner des objectifs abstraits en actions concrètes. Simultanément, une modélisation rigoureuse de l'incertitude, qu'elle provienne de la connaissance imparfaite de l'état du monde ou du caractère non déterministe des actions, est cruciale pour assurer la robustesse et l'adaptabilité des plans générés.

#### **2.1.b. Présentation des approches/algorithmes SOTA les plus pertinents**

##### **Extensions SOTA des Réseaux de Tâches Hiérarchiques (HTN) intégrant le raisonnement probabiliste ou la génération dynamique de "méthodes"**

* **Approche 1: Planification HTN en Environnement FOND (Fully Observable Non-Deterministic)**  
  * **i. Description Théorique :** Les Réseaux de Tâches Hiérarchiques (HTN) classiques offrent un cadre expressif pour la planification en décomposant des tâches complexes en sous-tâches plus simples via un ensemble de méthodes prédéfinies.1 Les extensions pour environnements FOND visent à intégrer la gestion de l'incertitude sur les résultats des actions primitives, où chaque action peut avoir plusieurs issues possibles, bien que l'état résultant soit entièrement observable. Le formalisme développé par Chen et Bercher (2022) introduit le concept de "method-based policies" (politiques basées sur les méthodes). Contrairement aux approches antérieures comme les "outcome-dependent fixed-method policies" (Chen et Bercher, 2021\) où la méthode de décomposition est fixée a priori mais son exécution s'adapte aux issues observées, les politiques basées sur les méthodes permettent de retarder le choix même de la méthode de décomposition jusqu'au moment de l'exécution, après avoir observé les résultats non déterministes des actions précédentes. Il en résulte que les solutions ne sont plus des réseaux de tâches statiques et complètement raffinés, mais plutôt des politiques complexes qui déterminent dynamiquement la séquence d'actions et les méthodes de décomposition en fonction des observations.1  
  * **ii. Avancées Récentes (Post-2022) :** S'appuyant sur ce cadre théorique, Chen, Olaya, et Bercher (IJCAI 2024\) ont présenté le premier système de planification capable de trouver des solutions fortes pour les problèmes HTN FOND.1 Une solution forte garantit l'atteinte de l'objectif en un temps fini, sans cycles, malgré le non-déterminisme des actions, tout en respectant la structure hiérarchique et l'ordonnancement partiel des tâches inhérents aux HTN. L'approche proposée inclut un algorithme de recherche spécifique et une technique de compilation qui relaxe le problème FOND HTN en un problème HTN déterministe. Cette relaxation ingénieuse permet d'exploiter les "grounders" (instanciateurs de problèmes) et les heuristiques existants issus de la littérature sur la planification HTN déterministe.1  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Cette approche vise la planification hiérarchique dans des contextes où les actions ont des effets multiples et possibles, mais où l'état du système reste entièrement observable après chaque action. L'objectif est de générer des politiques robustes qui garantissent l'accomplissement des tâches malgré ce non-déterminisme intrinsèque aux actions.  
    * **Limitations Conceptuelles :** La principale limitation réside dans l'hypothèse d'observabilité totale de l'état (FOND). Cette supposition peut s'avérer restrictive pour le modèle PI-QPM, qui mentionne explicitement un état "partiellement connu" et la gestion d'"hypothèses". La complexité de la recherche de solutions fortes, bien qu'analysée dans le travail de Chen et Bercher (2022) 1, peut demeurer élevée pour des problèmes de grande taille. De plus, bien que la sélection des méthodes soit dynamique, la génération des méthodes elles-mêmes reste guidée par des schémas de décomposition prédéfinis ; une génération *ab initio* de nouvelles méthodes de décomposition n'est pas directement adressée par ce formalisme.  
  * **iv. Références Clés :**  
    1. Chen, Y., Olaya, Á., & Bercher, P. (2024). HTN Planning with Non-Deterministic Actions: A Search for Strong Solutions. *Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI-24)*. 1  
    2. Chen, Y., & Bercher, P. (2022). FOND HTN Planning: A Complexity Analysis and an Approach Based on Method-Based Policies. *Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS)*, 32\. 1  
    3. Chen, Y., & Bercher, P. (2021). Towards HTN Planning with Non-Deterministic Action Outcomes. *Proceedings of the ICAPS Workshop on Hierarchical Planning (HPlan)*. 1  
* **Approche 2: Apprentissage de Domaines HTN (Méthodes et Actions) à partir d'Observations Partielles et Bruitées**  
  * **i. Description Théorique :** Plutôt que de s'appuyer sur des méthodes de décomposition entièrement prédéfinies, cette famille d'approches vise à apprendre ces méthodes, ainsi que les modèles d'actions, à partir de traces d'exécution ou d'observations du comportement d'un expert ou d'un système. L'algorithme HierAMLSI (Hierarchical Action Model Learning from Satisfied Implications), proposé par Grand et ses collaborateurs, s'inscrit dans cette lignée en utilisant des techniques d'induction grammaticale pour acquérir la connaissance du domaine de planification HTN, y compris les modèles d'action et les méthodes HTN avec leurs préconditions.2 Le principe fondamental repose sur l'apprentissage d'un automate fini déterministe (DFA) qui représente le langage des séquences de tâches valides, à partir d'un ensemble d'exemples positifs (séquences observées valides) et négatifs (séquences invalides).  
  * **ii. Avancées Récentes (Post-2022) :** Dans leur travail présenté à l'atelier HPlan 2022, Grand, Pellier, et Fiorino ont introduit HierAMLSI, une avancée notable capable d'apprendre simultanément les modèles d'actions et les méthodes HTN, même en présence d'observations d'entrée bruitées et partielles, tout en maintenant un haut niveau de précision.2 Cette capacité à gérer des données imparfaites est cruciale car elle réduit significativement la charge de travail des experts humains, qui est traditionnellement nécessaire pour définir ou corriger manuellement les domaines HTN. L'approche HierAMLSI se décompose en plusieurs étapes : génération d'observations (par exemple, via des marches aléatoires dans un simulateur), apprentissage d'un DFA pour les tâches primitives, induction d'un DFA de tâches intégrant les tâches composées, et enfin, extraction des méthodes HTN à partir de ce DFA. L'extraction des méthodes est formulée comme un problème de couverture d'ensemble (set cover problem), pour lequel une heuristique polynomiale est proposée afin de trouver un ensemble compact et pertinent de méthodes.3  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** L'acquisition automatique de modèles de domaine HTN (actions et méthodes de décomposition) dans des situations où la spécification manuelle est prohibitive, sujette à erreurs, ou lorsque les données disponibles pour l'apprentissage sont incomplètes ou bruitées.  
    * **Limitations Conceptuelles :** La qualité et la généralisabilité des méthodes apprises sont intrinsèquement dépendantes de la qualité, de la diversité et de la couverture des observations fournies en entrée. Bien que HierAMLSI intègre des mécanismes de raffinement pour gérer le bruit, une forte proportion de bruit ou des observations très parsemées peuvent impacter la précision. L'induction grammaticale, bien que puissante, peut être sensible à la complexité du langage cible et la généralisation à des situations radicalement différentes de celles observées durant l'apprentissage peut être limitée. L'expressivité des méthodes apprises est également contrainte par le formalisme du langage de description de domaine cible (par exemple, HDDL) et les capacités intrinsèques de l'algorithme d'induction.  
  * **iv. Références Clés :**  
    1. Grand, M., Pellier, D., & Fiorino, H. (2022). An Accurate HDDL Domain Learning Algorithm from Partial and Noisy Observations. *Proceedings of the 5th ICAPS Workshop on Hierarchical Planning (HPlan 2022\)*, 1–9. 2  
    2. Langley, P. (2025). Acquiring Hierarchical Knowledge for Planning. *To appear in Proceedings of the AAAI Conference on Artificial Intelligence*. 4 (Bien que prospectif, cet article de Langley cadre de manière pertinente la problématique générale de l'acquisition de connaissances hiérarchiques pour la planification).

##### **Algorithmes SOTA pour résoudre ou approximer des solutions à des Processus Décisionnels de Markov Partiellement Observables (POMDPs) de grande taille**

* **Approche 1: Solveurs POMDP en ligne basés sur l'échantillonnage avec garanties améliorées et gestion des récompenses dépendantes de la croyance**  
  * **i. Description Théorique :** Les Processus Décisionnels de Markov Partiellement Observables (POMDPs) constituent le cadre formel de référence pour la prise de décision séquentielle dans des environnements où l'état du système n'est pas directement observable et où les actions ont des effets stochastiques.6 La solution d'un POMDP est une politique qui mappe les états de croyance (distributions de probabilité sur les états réels) aux actions. La résolution exacte des POMDPs étant computationnellement intraitable pour la plupart des problèmes d'intérêt, les solveurs en ligne basés sur l'échantillonnage, tels que POMCP (Partially Observable Monte Carlo Planning) 8, construisent un arbre de recherche partiel à partir de l'état de croyance courant en simulant des trajectoires futures. Une extension importante, les ρPOMDPs, introduit des fonctions de récompense qui dépendent explicitement de l'état de croyance de l'agent (par exemple, une récompense pour la réduction de l'entropie de la croyance), ce qui est crucial pour les tâches de collecte active d'information.10 L'algorithme POMCPOW (POMCP with Observation Widening) adapte POMCP pour gérer des espaces d'actions et d'observations continus en utilisant une technique de "progressive widening" qui contrôle la croissance de l'arbre de recherche.8  
  * **ii. Avancées Récentes (Post-2022) :**  
    1. **ρPOMCPOW :** Zhitnikov et al. (arXiv 2025\) ont introduit ρPOMCPOW, un solveur en ligne de type "anytime" spécifiquement conçu pour les ρPOMDPs opérant dans des espaces d'états, d'actions et d'observations continus.10 Cet algorithme se distingue par sa capacité à raffiner dynamiquement les représentations de la croyance (maintenues sous forme de nuages de particules) avec des garanties formelles d'amélioration de la qualité de la solution au fil du temps de calcul alloué. De plus, il intègre une méthode efficace pour le calcul incrémental des récompenses dépendantes de la croyance, réduisant ainsi significativement le coût computationnel associé à ces fonctions de récompense souvent complexes (par exemple, basées sur l'entropie). Les résultats expérimentaux indiquent que ρPOMCPOW surpasse les solveurs état de l'art antérieurs tant en termes d'efficacité computationnelle que de qualité de la politique résultante, notamment pour des tâches de collecte d'information et de réduction d'incertitude.10  
    2. **Garanties Déterministes d'Optimalité "Anytime" pour POMDPs en ligne :** Lim et al. (arXiv 2023, mis à jour en 2025\) ont proposé une méthodologie pour dériver des bornes déterministes sur la sous-optimalité des solutions fournies par les solveurs POMDP en ligne tels que POMCP.8 En introduisant le concept de POMDPs simplifiés (abstractions de l'espace d'états ou d'observations) et en établissant un lien déterministe formel entre la solution du POMDP simplifié et celle du POMDP original, cette approche permet de certifier la qualité de la solution à n'importe quel point de l'exécution de l'algorithme. Ces bornes peuvent être utilisées pour guider plus efficacement l'exploration de l'arbre de recherche et pour établir des critères d'arrêt plus robustes, menant à une convergence garantie vers la solution optimale en temps fini pour les POMDPs discrets. Ces techniques peuvent être intégrées dans des algorithmes existants, donnant naissance à des variantes comme DB-POMCP (Deterministic Bounded POMCP).9  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Prise de décision séquentielle en ligne dans des environnements de grande taille, partiellement observables, potentiellement avec des variables d'état, d'action et d'observation continues. Un accent particulier est mis sur les scénarios où l'objectif de l'agent inclut la réduction explicite de son incertitude sur l'état du monde (cas des ρPOMDPs).  
    * **Limitations Conceptuelles :** Pour **ρPOMCPOW**, bien que l'algorithme offre des garanties d'amélioration progressive, la complexité de calcul peut rester substantielle pour des horizons de planification très longs ou des espaces de croyance particulièrement complexes. La qualité de l'approximation de l'état de croyance par un ensemble fini de particules demeure un facteur critique. Concernant les **garanties déterministes proposées par Lim et al.**, leur formalisation actuelle est principalement axée sur les POMDPs à espaces discrets. L'extension de ces garanties formelles robustes aux espaces continus représente un défi de recherche significatif. De plus, cette approche suppose un accès aux modèles de transition et d'observation du POMDP 9, ce qui peut ne pas être le cas pour certains mécanismes d'échantillonnage de type "boîte noire" utilisés par des solveurs comme la version originale de POMCPOW.  
  * **iv. Références Clés :**  
    1. Zhitnikov, A., Yu, K., Lim, G., Kochenderfer, M. J., & Indelman, V. (2025). Anytime Incremental ρPOMDP Planning in Continuous Spaces. *arXiv preprint arXiv:2502.02549*. 10  
    2. Lim, G., Yu, K., Zhitnikov, A., Kochenderfer, M. J., & Indelman, V. (2023). Anytime Deterministic Optimality Guarantees for Online POMDP Planning. *arXiv preprint arXiv:2310.01791*. (Version mise à jour en 2025 citée comme 9) 8  
    3. Sunberg, Z. N., & Kochenderfer, M. J. (2018). POMCPOW: An Online Algorithm for POMDPs with Continuous State, Action, and Observation Spaces. *Proceedings of Robotics: Science and Systems (RSS)*. 8  
* **Approche 2: Apprentissage par Renforcement Profond (DRL) pour les POMDPs**  
  * **i. Description Théorique :** Les approches d'Apprentissage par Renforcement Profond (DRL) visent à apprendre des politiques de décision directement à partir de l'interaction avec l'environnement (ou un simulateur), souvent en utilisant des réseaux de neurones profonds pour approximer les fonctions de valeur (critique), les politiques (acteur), ou les deux (acteur-critique), dans des espaces d'états et d'actions de grande dimension.13 Pour les POMDPs, où l'état n'est pas entièrement observable, les architectures DRL doivent typiquement intégrer un mécanisme pour gérer l'historique des observations afin d'estimer implicitement ou explicitement l'état de croyance. Des architectures récurrentes (comme les LSTMs ou GRUs) ou basées sur des transformeurs sont couramment employées à cette fin. Des algorithmes de DRL "model-based" comme MuZero 14 et ses dérivés apprennent un modèle interne de la dynamique de l'environnement, de la fonction de récompense et de la politique, et utilisent ce modèle pour la planification (souvent avec MCTS).  
  * **ii. Avancées Récentes (Post-2022) :**  
    1. **UniZero :** Pu et al. (arXiv 2024 / OpenReview) ont proposé UniZero, une architecture inspirée de MuZero qui utilise un modèle du monde basé sur un transformeur pour apprendre un espace latent partagé et désenchevêtré de l'historique implicite.15 Cette approche vise à surmonter certaines limitations des architectures MuZero précédentes, notamment en ce qui concerne la gestion de la mémoire à long terme et la scalabilité à des scénarios d'apprentissage multitâche. UniZero permet une utilisation plus complète des données de trajectoire pendant l'entraînement et intègre MCTS pour la planification dans l'espace latent appris. Les résultats montrent des performances supérieures sur des benchmarks nécessitant une mémoire à long terme et une meilleure scalabilité multitâche.17  
    2. **DRL pour POMDPs avec retour d'état bruité statique (SNSF) :** Ran et al. (IEEE TCDS 2025\) ont développé une technique d'estimation de la récompense pour dériver la fonction Q pour une classe spécifique de POMDPs où l'état initial est accessible, la politique est sans mémoire (statique), et la fonction de récompense peut être décomposée additivement en composantes dépendant de l'état et de l'action.7 Cette approche cible un sous-ensemble particulier de problèmes POMDP.  
    3. Les conférences majeures comme NeurIPS et ICML continuent de présenter des avancées pertinentes. Par exemple, des travaux sur "Model-based Reinforcement Learning for Confounded POMDPs" ont été présentés à ICML 2024, indiquant un intérêt soutenu pour les POMDPs avec des défis supplémentaires comme la présence de variables de confusion.18 La recherche générale en DRL continue également d'aborder des défis transversaux tels que l'efficacité de l'échantillon, la gestion des récompenses clairsemées et l'apprentissage multi-agents, qui sont tous pertinents pour la résolution de POMDPs complexes.13  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Apprentissage de politiques de décision performantes dans des POMDPs de grande dimension, potentiellement avec des dynamiques environnementales initialement inconnues (approches DRL "model-free") ou apprises par l'agent (approches DRL "model-based" comme UniZero).  
    * **Limitations Conceptuelles :** Pour **UniZero**, bien que prometteur pour la mémoire à long terme et le multitâche, la complexité de l'entraînement des modèles basés sur des transformeurs et la stabilité de l'apprentissage du modèle du monde restent des défis techniques importants. Les garanties théoriques de convergence vers une politique optimale sont généralement plus faibles que celles des solveurs basés sur l'échantillonnage qui opèrent avec des modèles connus ou des hypothèses plus fortes. Pour le **DRL en général appliqué aux POMDPs**, l'efficacité de l'échantillon ("sample efficiency") demeure une préoccupation majeure, nécessitant souvent une quantité considérable d'interactions pour converger. La conception de fonctions de récompense informatives est critique et peut s'avérer ardue. La robustesse des politiques apprises face à des changements dans la distribution des observations ou des dynamiques entre la phase d'entraînement et la phase de déploiement (problème de "distribution shift") est une autre limitation. Enfin, l'interprétabilité des politiques complexes apprises par les réseaux de neurones profonds reste limitée.  
  * **iv. Références Clés :**  
    1. Pu, Y., Niu, Y., Yang, Z., Ren, J., Li, H., & Liu, Y. (2024). UniZero: Generalized and Efficient Planning with Scalable Latent World Models. *arXiv preprint arXiv:2406.10667*. (Accepté à NeurIPS 2024 selon 16) 15  
    2. Ran, K., Su, H., Wu, C., & Xu, X. (2025). Deep reinforcement learning for static noisy state feedback. *IEEE Transactions on Cognitive and Developmental Systems*. 7  
    3. Schrittwieser, J., Antonoglou, I., Hubert, T., et al. (2020). Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. *Nature*, 588(7839), 604–609. (Article fondateur de MuZero, base pour UniZero) 14

Le modèle PI-QPM, de par sa nature, semble exiger à la fois une structuration hiérarchique des "chemins d'exécution" contenus dans son "Graphe de Potentiel Limité" et une gestion fine de l'incertitude découlant de l'observation partielle et des probabilités associées aux hypothèses. Cette double exigence met en lumière une tension potentielle entre les approches HTN et POMDP. Les formalismes HTN FOND, tels que ceux développés par Chen et ses collaborateurs 1, offrent des capacités de planification hiérarchique robuste face au non-déterminisme des actions, mais leur cadre théorique repose sur l'hypothèse d'une observabilité totale de l'état. À l'inverse, les POMDPs et leurs solveurs état de l'art (comme ρPOMCPOW 10 ou les approches DRL comme UniZero 15) sont nativement conçus pour gérer l'observabilité partielle et l'incertitude sur l'état. Cependant, l'intégration d'une décomposition hiérarchique explicite, avec la sémantique riche des méthodes HTN, au sein de ces solveurs POMDP n'est pas directe et constitue un champ de recherche actif. L'apprentissage de méthodes HTN 2 pourrait générer les structures hiérarchiques nécessaires, mais leur couplage opérationnel avec un solveur POMDP pour une exécution robuste sous observation partielle reste un défi conceptuel. Une direction de recherche prometteuse pour un système comme PI-QPM serait donc d'explorer des formalismes hybrides ou des intégrations plus poussées entre la planification hiérarchique, pour la structuration et la décomposition des problèmes complexes, et la planification POMDP, pour une gestion rigoureuse de l'incertitude liée à l'observation et à l'action. Cela pourrait, par exemple, impliquer l'utilisation de méthodes HTN pour générer des macro-actions candidates ou des politiques de haut niveau qui seraient ensuite évaluées et exécutées par un solveur POMDP opérant à un niveau plus fin.

Le tableau suivant synthétise les approches discutées pour ce pilier.

**Tableau 1.1: Synopsis des Approches SOTA pour la Planification et Décomposition sous Incertitude**

| Approche/Algorithme | Principe Théorique | Avancées Récentes (Post-2022) | Problème Ciblé | Limitations Conceptuelles | Références Clés |
| :---- | :---- | :---- | :---- | :---- | :---- |
| Planification HTN FOND | Décomposition hiérarchique avec actions non déterministes, politiques basées sur les méthodes. | Premier système de planification pour HTN FOND trouvant des solutions fortes (Chen et al., IJCAI 2024). Relaxation vers HTN déterministe. 1 | Planification hiérarchique avec non-déterminisme des actions et observabilité totale. | Observabilité totale (FOND). Complexité potentielle. Génération de méthodes limitée aux décompositions prédéfinies. | Chen et al. (IJCAI 2024\) 1, Chen & Bercher (ICAPS 2022\) 1 |
| Apprentissage de Domaines HTN (HierAMLSI) | Induction grammaticale (apprentissage de DFA) pour acquérir actions et méthodes HTN à partir d'observations. | HierAMLSI (Grand et al., HPlan 2022\) apprend avec observations partielles et bruitées. Extraction de méthodes via heuristique polynomiale. 2 | Acquisition automatique de modèles de domaine HTN à partir de données imparfaites. | Dépendance à la qualité/couverture des observations. Sensibilité au bruit élevé. Généralisation limitée. | Grand et al. (HPlan 2022\) 2, Langley (AAAI 2025\) 4 |
| Solveurs POMDP en ligne (Échantillonnage) | Construction d'arbre de recherche partiel (MCTS-like) depuis l'état de croyance. ρPOMDPs pour récompenses dépendantes de la croyance. | ρPOMCPOW (Zhitnikov et al., arXiv 2025\) pour ρPOMDPs continus, raffinement dynamique de croyance, calcul incrémental des récompenses..10 Garanties déterministes d'optimalité "anytime" pour POMDPs discrets (Lim et al., arXiv 2023). 8 | Décision en ligne en environnement vaste, partiellement observable, potentiellement continu, avec objectifs de réduction d'incertitude. | ρPOMCPOW: complexité, qualité de l'approximation de croyance. Garanties Lim et al.: focus sur discret, suppose accès aux modèles. | Zhitnikov et al. (arXiv 2025\) 10, Lim et al. (arXiv 2023\) 8, Sunberg & Kochenderfer (RSS 2018\) 8 |
| DRL pour POMDPs | Apprentissage de politiques/fonctions de valeur via réseaux de neurones, gestion de l'historique pour estimer la croyance. Modèles appris (MuZero-style). | UniZero (Pu et al., arXiv 2024): modèle du monde basé sur transformeur pour mémoire longue et multitâche..15 DRL pour POMDPs SNSF (Ran et al., TCDS 2025). 7 | Apprentissage de politiques en POMDPs de grande dimension, dynamiques potentiellement inconnues. | UniZero: complexité d'entraînement, stabilité, garanties théoriques plus faibles. DRL général: efficacité d'échantillon, conception de récompense, robustesse, interprétabilité. | Pu et al. (arXiv 2024\) 15, Ran et al. (TCDS 2025\) 7, Schrittwieser et al. (Nature 2020\) 14 |

### **Pilier 2 : Stratégies d'Exploration et de Sélection d'Actions**

#### **2.2.a. Introduction au problème théorique du pilier**

Ce pilier se concentre sur les mécanismes algorithmiques permettant aux agents du système PI-QPM de naviguer intelligemment dans un vaste espace de décision afin de sélectionner des actions optimales ou informationnellement pertinentes. Cette problématique est directement liée aux concepts d'"Exploration Itérative à Gain Décroissant" et d'"Exécution par Point de Levier" stipulés dans la description du PI-QPM. Il s'agit de trouver un équilibre dynamique et fondé entre l'exploitation des stratégies actuellement jugées les plus prometteuses (en termes de coût, risque, valeur) et l'exploration active de l'environnement ou du graphe de potentiel. L'exploration vise ici à acquérir de nouvelles informations, à réduire l'incertitude sur les hypothèses du système et à potentiellement découvrir de meilleures stratégies. La complexité est accrue par le fait que les actions peuvent avoir des coûts variables et que les récompenses (ou la confirmation des hypothèses) peuvent être significativement retardées.

#### **2.2.b. Présentation des approches/algorithmes SOTA les plus pertinents**

##### **Adaptations théoriques SOTA de la Recherche d'Arbre Monte-Carlo (MCTS) pour des problèmes de planification générale**

* **Approche 1: Cost-Augmented MCTS (CATS)**  
  * **i. Description Théorique :** La Recherche d'Arbre Monte-Carlo (MCTS) est un algorithme de recherche heuristique particulièrement efficace pour les problèmes de décision séquentielle, notamment ceux avec de grands espaces d'états et d'actions. Son principe repose sur la construction itérative d'un arbre de recherche, où chaque nœud représente un état et chaque arête une action. MCTS équilibre l'exploration de nouvelles séquences d'actions et l'exploitation des séquences les plus prometteuses découvertes jusqu'à présent. Les quatre étapes fondamentales de MCTS sont : la Sélection (choix d'un chemin dans l'arbre existant, souvent guidé par le critère UCT \- Upper Confidence Bound for Trees), l'Expansion (ajout d'un nouveau nœud à l'arbre), la Simulation (ou "rollout", exécution d'une séquence d'actions aléatoires ou guidées par une politique par défaut jusqu'à un état terminal pour estimer la valeur du nouveau nœud), et la Rétropropagation (mise à jour des statistiques des nœuds sur le chemin sélectionné avec le résultat de la simulation).19 L'approche Cost-Augmented MCTS (CATS) étend ce cadre classique en intégrant explicitement la notion de coût des actions et de contraintes budgétaires dans le processus de recherche.19  
  * **ii. Avancées Récentes (Post-2022) :** Zhang et Liu (arXiv 2025\) ont proposé CATS, initialement dans le contexte de la planification assistée par des Grands Modèles de Langage (LLM), mais dont le principe d'augmentation par les coûts est généralisable à d'autres domaines de planification.19 Dans CATS, les actions sont pondérées non seulement par leur valeur estimée (récompense) mais aussi par leur coût. De plus, l'algorithme est conçu pour élaguer activement les chemins de l'arbre de recherche qui deviennent infaisables au regard d'une contrainte budgétaire globale. L'objectif est de trouver un équilibre entre l'accomplissement de la tâche (maximisation de la récompense) et le respect des contraintes de coût, en priorisant les chemins à faible coût et haute récompense.19  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Planification séquentielle où les actions ont des coûts explicites et hétérogènes, et où le décideur doit opérer sous des contraintes budgétaires globales, tout en cherchant à maximiser des récompenses qui peuvent être retardées.  
    * **Limitations Conceptuelles :** L'intégration des coûts dans le critère de sélection de MCTS (par exemple, une version modifiée d'UCT) nécessite une formulation théorique soignée pour éviter de biaiser excessivement l'exploration au détriment de séquences d'actions potentiellement très récompensantes mais initialement coûteuses. La gestion des budgets globaux peut complexifier l'élagage de l'arbre et l'estimation de la valeur des nœuds, car la faisabilité d'un chemin dépend de l'historique des coûts accumulés. La version de CATS présentée par Zhang et Liu 19 s'appuie sur des LLMs pour la proposition d'actions et la modélisation du monde, ce qui pourrait nécessiter des adaptations conceptuelles pour une application directe dans des contextes où un LLM n'est pas le modèle de monde principal, comme potentiellement dans PI-QPM.  
  * **iv. Références Clés :**  
    1. Zhang, Z., & Liu, F. (2025). Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning. *arXiv preprint arXiv:2505.14656*. 19  
    2. Kocsis, L., & Szepesvári, C. (2006). Bandit based Monte-Carlo Planning. *Proceedings of the European Conference on Machine Learning (ECML)*, 282–293. (Article fondateur pour UCT dans MCTS).  
    3. Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P.,... & Colton, S. (2012). A survey of monte carlo tree search methods. *IEEE Transactions on Computational Intelligence and AI in games*, 4(1), 1-43. (Survey de référence sur MCTS).  
* **Approche 2: MCTS dans les modèles du monde appris (par exemple, MuZero, UniZero)**  
  * **i. Description Théorique :** Des algorithmes d'apprentissage par renforcement profond comme MuZero 14 et son successeur UniZero 15 combinent MCTS avec un modèle du monde qui est entièrement appris à partir de l'expérience. Ce modèle appris inclut typiquement une fonction de dynamique (prédisant l'état latent suivant), une fonction de récompense (prédisant la récompense immédiate) et une fonction de politique (prédisant une politique prioritaire). MCTS est ensuite déployé pour planifier dans l'espace latent de ce modèle appris, lui permettant de "regarder vers l'avant" et d'évaluer les conséquences de différentes séquences d'actions sans interagir directement avec l'environnement réel à chaque étape de la simulation interne. Les récompenses peuvent être retardées, et les "coûts" des actions peuvent être implicitement capturés et appris par la fonction de valeur ou par la dynamique du modèle (par exemple, une action coûteuse pourrait mener à des états de faible valeur ou consommer une ressource modélisée).  
  * **ii. Avancées Récentes (Post-2022) :** UniZero, proposé par Pu et al. (2024), représente une avancée significative par rapport à MuZero en utilisant une architecture basée sur un transformeur pour son modèle du monde.15 Cette innovation architecturale confère à UniZero une meilleure capacité à gérer la mémoire à long terme (cruciale pour les dépendances temporelles étendues) et une scalabilité accrue pour l'apprentissage multitâche. MCTS opère en utilisant les prédictions de ce modèle transformeur (état latent suivant, récompense, distribution de politique) pour guider sa recherche. Bien que la gestion des coûts d'action variables ne soit pas un mécanisme explicite et distinct dans UniZero, ces coûts peuvent être encodés dans les signaux de récompense que le modèle apprend à prédire.  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Planification dans des environnements complexes où les dynamiques et les fonctions de récompense sont initialement inconnues et doivent être apprises à partir des interactions. Convient particulièrement aux problèmes avec des récompenses retardées et des dépendances complexes.  
    * **Limitations Conceptuelles :** La performance globale de MCTS dans ce cadre dépend de manière critique de la qualité et de la précision du modèle du monde appris. Des erreurs ou des imprécisions dans le modèle appris peuvent conduire MCTS à élaborer des plans sous-optimaux ou erronés. L'intégration explicite de coûts d'action variables et de contraintes budgétaires strictes peut être moins directe et moins contrôlable que dans des approches comme CATS, car elle repose sur la capacité du modèle à inférer et à représenter correctement ces aspects uniquement à partir des signaux de récompense observés. Les garanties théoriques de convergence vers l'optimalité sont souvent liées à la convergence de l'apprentissage du modèle et de la politique sous-jacente, plutôt qu'à l'optimalité de l'algorithme MCTS opérant au sein d'un modèle parfait.  
  * **iv. Références Clés :**  
    1. Pu, Y., Niu, Y., Yang, Z., Ren, J., Li, H., & Liu, Y. (2024). UniZero: Generalized and Efficient Planning with Scalable Latent World Models. *arXiv preprint arXiv:2406.10667*. 15  
    2. Schrittwieser, J., Antonoglou, I., Hubert, T., et al. (2020). Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. *Nature*, 588(7839), 604–609. 14 (Article fondateur de MuZero).  
    3. Silver, D., & Veness, J. (2010). Monte-Carlo planning in large POMDPs. *Advances in neural information processing systems*, 23\. (POMCP, une base pour de nombreux travaux MCTS sous incertitude).

##### **Formalismes SOTA issus de la théorie de l'information pour guider la sélection d'actions afin de maximiser le gain d'information**

* **Approche 1: Optimisation Bayésienne (BO) et Apprentissage Actif (AL) pour la sélection guidée par l'information**  
  * **i. Description Théorique :** L'Optimisation Bayésienne (BO) est une méthodologie d'optimisation globale conçue pour des fonctions "boîte-noire" dont l'évaluation est coûteuse.22 Elle opère en deux phases itératives : premièrement, elle construit un modèle de substitution probabiliste (souvent un Processus Gaussien \- GP) qui représente la croyance actuelle sur la fonction objectif, en se basant sur les points déjà évalués. Deuxièmement, elle utilise une fonction d'acquisition pour déterminer le prochain point à évaluer. Cette fonction d'acquisition quantifie l'utilité d'évaluer la fonction objectif en un nouveau point, en équilibrant l'exploitation (choisir des points où le modèle prédit une haute valeur) et l'exploration (choisir des points où l'incertitude du modèle est élevée). Certaines fonctions d'acquisition, comme l'Upper Confidence Bound (UCB) ou l'Entropy Search, visent explicitement la réduction de l'incertitude. L'Apprentissage Actif (AL), dans un contexte de classification ou de régression, sélectionne les points de données non étiquetés les plus informatifs à soumettre à un oracle (par exemple, un expert humain) pour étiquetage, afin d'améliorer le modèle avec un minimum d'effort.22 Il existe une dualité conceptuelle forte entre les critères de remplissage ("infill criteria") de la BO et les critères d'apprentissage en AL, tous deux cherchant à maximiser l'information acquise par échantillon.22  
  * **ii. Avancées Récentes (Post-2022) :** La recherche continue d'explorer et d'exploiter la synergie entre BO et AL, comme souligné par Miriyala et al. (arXiv 2023).22 Les développements récents se concentrent sur la conception de fonctions d'acquisition plus sophistiquées, capables de gérer des objectifs multiples ou des contraintes complexes, l'intégration d'informations provenant de sources multiples avec différents niveaux de fidélité (multi-fidelity Bayesian optimization), et l'application de ces techniques à des espaces de décision de très grande dimension. Des cadres logiciels modernes comme BoTorch 23 (mentionné dans un document de PhysicsX AI, 2024\) illustrent cette tendance en fournissant des implémentations flexibles de fonctions d'acquisition basées sur des méthodes de Monte Carlo (MC). Ces fonctions MC sont souvent préférées aux fonctions analytiques en raison de leur plus grande adaptabilité et de leur capacité à gérer des modèles de substitution complexes. Elles utilisent des techniques d'échantillonnage avancées, comme les séquences de Sobol, pour réduire la variance de l'estimation et permettre l'optimisation par gradient de la fonction d'acquisition elle-même.23  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Sélectionner séquentiellement des actions (ou des configurations à évaluer, des expériences à mener) dans le but d'optimiser un objectif (par exemple, la performance d'un plan, la valeur d'une stratégie) tout en apprenant simultanément un modèle du monde sous-jacent ou de la fonction objectif elle-même. Un objectif clé est de réduire l'incertitude sur ce modèle ou sur la localisation de l'optimum avec un nombre minimal d'évaluations coûteuses.  
    * **Limitations Conceptuelles :** L'Optimisation Bayésienne peut être computationnellement intensive, en particulier l'étape de mise à jour du modèle de substitution (par exemple, l'inversion de la matrice de covariance pour un GP) et l'optimisation globale de la fonction d'acquisition, surtout lorsque l'espace de décision est de grande dimension. La performance de la BO dépend de manière critique du choix approprié du modèle de substitution (par exemple, le noyau du GP) et de la fonction d'acquisition, choix qui peuvent nécessiter une expertise du domaine. L'extension de la BO à des espaces d'actions structurés, discrets ou combinatoires, typiques des problèmes de planification, reste un domaine de recherche actif et constitue un défi par rapport aux espaces continus plus classiquement abordés.  
  * **iv. Références Clés :**  
    1. Frazier, P. I. (2018). A Tutorial on Bayesian Optimization. *arXiv preprint arXiv:1807.02811*. (Tutoriel fondamental et complet sur la BO).  
    2. Miriyala, S. S., Cazzolato, B. S., & Zaeemnia, P. (2023). A unified perspective of Bayesian optimization and active learning. *arXiv preprint arXiv:2303.01560*. 22  
    3. Balandat, M., Karrer, B., Jiang, D. R., Daulton, S., Letham, B., Fowler, M. A., & Bakshy, E. (2020). BoTorch: A Framework for Bayesian Optimization in PyTorch. *Advances in Neural Information Processing Systems*, 33\. (Référence pour BoTorch, citée dans 23).  
* **Approche 2: Exploration Information-Théorique en Apprentissage par Renforcement (Model-Free)**  
  * **i. Description Théorique :** Cette approche, distincte de la BO qui est souvent "model-based" (via le surrogate), vise à concevoir des stratégies d'exploration directement au sein des algorithmes d'apprentissage par renforcement "model-free". L'objectif est de minimiser le nombre d'échantillons (interactions avec l'environnement) nécessaires pour identifier une politique quasi-optimale, un problème connu sous le nom de "Best Policy Identification" (BPI).24 L'approche s'appuie sur des fondements de la théorie de l'information, en partant d'une borne inférieure spécifique à l'instance (instance-specific lower bound) sur la complexité d'échantillonnage. Cette borne quantifie le nombre minimum d'échantillons requis et dépend de quantités information-théoriques liées à la difficulté de distinguer la politique optimale des politiques sous-optimales.  
  * **ii. Avancées Récentes (Post-2022) :** Mutti, Restelli, et Tirinzoni (NeurIPS 2023\) ont proposé une solution "model-free" innovante pour ce problème.24 Leur contribution majeure est de dériver une approximation de la borne inférieure spécifique à l'instance qui ne fait intervenir que des quantités estimables sans avoir besoin de construire un modèle explicite de l'environnement. Ces quantités sont typiquement la fonction Q (valeur état-action) et la variance de la fonction de valeur, toutes deux inférables via des méthodes d'approximation stochastique courantes en RL. Pour gérer l'incertitude paramétrique inhérente à l'estimation de ces quantités durant l'apprentissage, ils emploient une méthode basée sur un ensemble (ensemble-based) combinée à des techniques de bootstrapping. Cette approche permet de quantifier l'incertitude et de guider l'exploration de manière adaptative, en fonction de la difficulté d'apprentissage estimée pour l'MDP spécifique.  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Réaliser une exploration efficace dans des Processus Décisionnels de Markov (MDPs), potentiellement à espaces continus, afin d'apprendre rapidement une politique performante. L'accent est mis sur la réduction de l'incertitude qui est la plus pertinente pour discriminer la meilleure politique des autres.  
    * **Limitations Conceptuelles :** L'efficacité de l'approche repose sur la qualité de l'approximation de la borne inférieure et sur la précision de l'estimation des quantités nécessaires (fonction Q, variance de la fonction de valeur), ce qui peut introduire des erreurs d'approximation. La méthode "ensemble-based" avec bootstrapping, bien que robuste, ajoute une complexité computationnelle non négligeable par rapport aux stratégies d'exploration plus simples. Bien qu'étant "model-free" dans le sens où elle n'apprend pas un modèle de transition explicite, elle s'appuie fortement sur l'estimation de fonctions de valeur, dont la qualité peut varier en fonction de l'algorithme d'apprentissage sous-jacent et de la complexité du problème. La connexion directe entre cette approche (qui se concentre sur l'optimalité de la politique) et l'objectif de maximisation du "gain d'information" sur la structure du "Graphe de Potentiel Limité" de PI-QPM nécessiterait une adaptation conceptuelle pour aligner les objectifs d'information.  
  * **iv. Références Clés :**  
    1. Mutti, M., Restelli, M., & Tirinzoni, A. (2023). Learning to Explore Difficult MDPs with Information-Directed Model-Free Reinforcement Learning. *Advances in Neural Information Processing Systems (NeurIPS)*, 36\. 24  
    2. Russo, D., & Van Roy, B. (2014). Learning to optimize via posterior sampling. *Mathematics of Operations Research*, 39(4), 1221-1243. (Travail influent sur l'échantillonnage postérieur, une inspiration pour les techniques de bootstrapping, cité dans 25).  
    3. Kaufmann, E., Cappé, O., & Garivier, A. (2012). On Bayesian upper confidence bounds for bandit problems. *Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS)*. (Référence pour les approches bayésiennes et UCB dans les bandits, contexte lié à l'exploration).

Un thème central et puissant émerge de l'analyse des approches SOTA pour ce pilier : la quantification de l'information, ou plus précisément de la réduction de l'incertitude, comme un guide fondamental pour l'exploration. Le modèle PI-QPM, avec ses principes d'"Exploration Itérative à Gain Décroissant" et d'"Exécution par Point de Levier" qui maximise le gain d'information, s'aligne parfaitement avec cette tendance. Que ce soit à travers les récompenses dépendantes de la croyance dans les ρPOMDPs (où l'agent peut être explicitement récompensé pour des actions qui réduisent l'entropie de son état de croyance 10), les fonctions d'acquisition en Optimisation Bayésienne et Apprentissage Actif (qui sélectionnent les points maximisant l'information attendue ou réduisant l'incertitude du modèle 22), ou encore les approches d'Apprentissage par Renforcement guidées par la théorie de l'information (qui cherchent à minimiser le nombre d'échantillons pour identifier la meilleure politique en se concentrant sur les aspects les plus incertains et discriminants 25), l'idée sous-jacente est de traiter l'information comme une ressource précieuse, une "monnaie" d'exploration. Ces diverses approches fournissent des formalismes mathématiques robustes pour quantifier ce "gain d'information" et pour optimiser les stratégies de sélection d'actions en conséquence. Pour PI-QPM, cela implique la nécessité d'adopter ou d'adapter l'un de ces formalismes pour rendre opérationnelle sa stratégie d'exploration. Un point d'intégration crucial sera la manière dont l'information acquise par ces actions exploratoires est utilisée pour la "Mise à Jour Probabiliste" du "Graphe de Potentiel Limité".

Le tableau suivant résume les approches SOTA pour ce pilier.

**Tableau 2.1: Synopsis des Approches SOTA pour les Stratégies d'Exploration et de Sélection d'Actions**

| Approche/Algorithme | Principe Théorique | Avancées Récentes (Post-2022) | Problème Ciblé | Limitations Conceptuelles | Références Clés |
| :---- | :---- | :---- | :---- | :---- | :---- |
| Cost-Augmented MCTS (CATS) | MCTS avec intégration explicite des coûts d'action et contraintes budgétaires. Pondération et élagage basés sur les coûts. | CATS (Zhang & Liu, arXiv 2025\) pour planification assistée par LLM, généralisable. 19 | Planification avec coûts d'action variables, budgets, et récompenses retardées. | Formulation du critère de sélection (ex: UCT augmenté). Gestion des budgets globaux. Dépendance au LLM dans la version présentée. | Zhang & Liu (arXiv 2025\) 19, Kocsis & Szepesvári (ECML 2006\) |
| MCTS dans les modèles appris (UniZero) | MCTS planifie dans l'espace latent d'un modèle du monde (dynamique, récompense, politique) appris par DRL. | UniZero (Pu et al., arXiv 2024\) utilise un transformeur pour le modèle du monde, améliorant mémoire longue et multitâche. 15 | Planification en environnement complexe avec dynamiques inconnues et récompenses retardées. | Dépendance à la qualité du modèle appris. Intégration de coûts/budgets explicites moins directe. Garanties théoriques liées à l'apprentissage. | Pu et al. (arXiv 2024\) 15, Schrittwieser et al. (Nature 2020\) 14 |
| Optimisation Bayésienne (BO) / Apprentissage Actif (AL) pour gain d'information | Modèle de substitution probabiliste et fonction d'acquisition pour sélectionner des évaluations maximisant l'information ou l'amélioration. | Fonctions d'acquisition MC flexibles (BoTorch). Synergie BO/AL explorée (Miriyala et al., arXiv 2023). 22 | Sélection séquentielle d'actions/expériences pour optimiser un objectif tout en apprenant un modèle et réduisant l'incertitude. | Coût calculatoire (mise à jour du surrogate, optimisation de l'acquisition). Dépendance au choix du surrogate/noyau et de la fonction d'acquisition. Extension aux espaces structurés. | Frazier (arXiv 2018), Miriyala et al. (arXiv 2023\) 22, Balandat et al. (NeurIPS 2020\) 23 |
| Exploration Information-Théorique en RL (Model-Free) | Stratégies d'exploration minimisant les échantillons pour identifier la politique optimale, basées sur des bornes inférieures de complexité d'échantillonnage. | Approximation model-free de la borne inférieure (Mutti et al., NeurIPS 2023\) utilisant Q-fonction et variance de V, avec ensemble-based bootstrapping. 24 | Exploration efficace en MDPs pour apprentissage rapide de politique, focalisée sur la réduction d'incertitude pertinente pour BPI. | Erreurs d'approximation (borne, Q, V). Complexité de l'ensemble-based. Connexion au gain d'info sur la structure du KG de PI-QPM à adapter. | Mutti et al. (NeurIPS 2023\) 24, Russo & Van Roy (MOR 2014\) 25 |

### **Pilier 3 : Théorie de la Décision Multi-Objectifs**

#### **2.3.a. Introduction au problème théorique du pilier**

Le concept de "Jugement Objectif" au sein du modèle PI-QPM implique une évaluation rigoureuse de la "bonté" d'un plan ou d'une stratégie selon une pluralité de métriques quantifiables, telles que le coût, le risque, et la valeur. Ces métriques sont fréquemment en conflit : par exemple, un plan minimisant le coût pourrait maximiser le risque, ou un plan maximisant la valeur pourrait nécessiter un coût prohibitif. L'Optimisation Multi-Objectifs (MOO) fournit l'arsenal théorique et algorithmique pour aborder de tels problèmes. Son objectif principal n'est pas de trouver une unique solution "optimale", mais plutôt d'identifier ou d'approximer l'ensemble des solutions de compromis optimales, connu sous le nom de Front de Pareto. Une solution est dite Pareto-optimale si aucun objectif ne peut être amélioré sans en dégrader au moins un autre. Ce pilier se penche sur les algorithmes état de l'art capables d'identifier ou d'approximer ces fronts de Pareto, en particulier dans des espaces de décision de grande dimension ou dynamiques. Il examine également les métriques SOTA utilisées pour évaluer la qualité (convergence et diversité) d'un ensemble de solutions de Pareto et pour mesurer la progression au cours d'un processus d'optimisation itératif, ce qui est crucial pour "l'Exploration Itérative à Gain Décroissant" de PI-QPM.

#### **2.3.b. Présentation des approches/algorithmes SOTA les plus pertinents**

##### **Algorithmes SOTA en Optimisation Multi-Objectifs (MOO) pour identifier ou approximer le Front de Pareto**

* **Approche 1: Algorithmes Évolutionnaires Multi-Objectifs (MOEAs) Basés sur la Décomposition et la Référence**  
  * **i. Description Théorique :** Les Algorithmes Évolutionnaires Multi-Objectifs (MOEAs) constituent une classe prédominante d'heuristiques de recherche pour les problèmes MOO, inspirées par les principes de l'évolution naturelle. Parmi les MOEAs les plus influents, MOEA/D (Multi-Objective Evolutionary Algorithm based on Decomposition) 26 aborde le problème MOO en le décomposant en un ensemble de sous-problèmes d'optimisation scalaires. Chaque sous-problème est défini par un vecteur de poids ou un point de référence, et l'algorithme fait évoluer une population de solutions où chaque solution est associée à un sous-problème. L'optimisation se fait en considérant les solutions voisines dans l'espace des vecteurs de poids. D'autre part, NSGA-III (Non-dominated Sorting Genetic Algorithm III) 27 est une extension de NSGA-II conçue spécifiquement pour les problèmes avec un grand nombre d'objectifs (many-objective optimization). NSGA-III utilise un ensemble de points de référence distribués de manière structurée dans l'espace objectif pour maintenir la diversité des solutions le long du front de Pareto et pour guider la sélection.  
  * **ii. Avancées Récentes (Post-2022) :**  
    1. **Améliorations de NSGA-III et MOEA/D par de Nouvelles Stratégies de Recherche :** Gao et al. (PLoS ONE, via PMC, 2024\) ont proposé une nouvelle stratégie de recherche, baptisée "NG strategy", qui combine une "stratégie voisine" (neighbor strategy) et une "stratégie de guidage" (guidance strategy) pour améliorer l'efficacité de recherche des algorithmes NSGA-III et MOEA/D.27 La stratégie voisine contraint la sélection des partenaires de croisement à un voisinage défini par rapport aux points de référence, favorisant ainsi l'exploitation locale. La stratégie de guidage, quant à elle, utilise périodiquement des individus qui sont optimaux sur un unique objectif comme partenaires de croisement, afin d'aider la population à échapper aux optima locaux et à explorer de nouvelles régions. L'application de cette stratégie NG a conduit aux variantes NSGA-III/NG et MOEA/D-NG, qui ont démontré des améliorations significatives en termes de vitesse de convergence (environ 12.54%) et de précision de l'ensemble des solutions non dominées obtenues (environ 3.67%) sur des problèmes benchmarks.27  
    2. **Benchmarking et Analyse "Anytime" :** Des travaux récents sur le benchmarking des MOEAs, comme celui de de Nobel et al. (arXiv 2024), soulignent l'importance croissante de l'analyse "anytime" de la performance des algorithmes.28 Cela implique d'évaluer la qualité du front de Pareto approximé à différents stades de l'exécution de l'algorithme, plutôt qu'uniquement à la fin. L'utilisation d'archives non bornées pour stocker toutes les solutions non dominées rencontrées au cours de la recherche est préconisée pour permettre une telle analyse flexible et l'application de divers indicateurs de performance de manière monotone.28  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Trouver un ensemble de solutions diversifié et bien réparti qui approxime fidèlement le vrai Front de Pareto pour des problèmes d'optimisation multi-objectifs complexes. Cela inclut les problèmes avec un grand nombre d'objectifs (many-objective) ou des espaces de décision de très grande dimension.  
    * **Limitations Conceptuelles :** Pour **NSGA-III/NG et MOEA/D-NG**, bien qu'ils améliorent les performances des versions originales, leur efficacité reste dépendante de paramètres cruciaux tels que le nombre et la distribution des points de référence (pour NSGA-III) ou des vecteurs de poids (pour MOEA/D). La performance peut également être affectée par la forme géométrique du vrai front de Pareto (par exemple, fronts dégénérés, discontinus ou avec des formes complexes). Les garanties de convergence vers le vrai front de Pareto sont de nature probabiliste et asymptotique. La gestion efficace des espaces de décision dynamiques, où les fonctions objectifs ou les contraintes évoluent avec le temps, demeure un défi conceptuel majeur pour la plupart des MOEAs standards et nécessite souvent des adaptations algorithmiques spécifiques non triviales.  
  * **iv. Références Clés :**  
    1. Gao, Y., Wang, G., Wang, H., & Ma, K. (2024). New search strategy for multi-objective evolutionary algorithm. *PLoS ONE*, 19(12), e0314227. (via PMC) 27  
    2. Deb, K., & Jain, H. (2014). An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints. *IEEE Transactions on Evolutionary Computation*, 18(4), 577-601. (Article original sur NSGA-III).  
    3. Zhang, Q., & Li, H. (2007). MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition. *IEEE Transactions on Evolutionary Computation*, 11(6), 712-731. 26 (Article original sur MOEA/D).  
* **Approche 2: Algorithmes de Qualité-Diversité Multi-Objectifs (MOQD)**  
  * **i. Description Théorique :** Les algorithmes de Qualité-Diversité (QD) représentent une branche de l'optimisation évolutionnaire qui vise non seulement à trouver des solutions performantes (qualité) mais aussi à découvrir un ensemble de solutions qui sont comportementalement diverses, c'est-à-dire qui accomplissent la tâche de manières qualitativement différentes. Les algorithmes MOQD (Multi-Objective Quality-Diversity) étendent ce paradigme aux problèmes impliquant plusieurs objectifs conflictuels.29 L'objectif est de trouver, pour chaque "niche" comportementale définie dans un espace de caractéristiques (behavioural descriptor space), un ensemble de solutions qui sont Pareto-optimales par rapport aux objectifs de performance.  
  * **ii. Avancées Récentes (Post-2022) :**  
    1. **MOUR-QD (Multi-Objective Unstructured Repertoire for Quality-Diversity) :** Janmohamed et Cully (arXiv 2025\) ont introduit MOUR-QD, un algorithme MOQD spécifiquement conçu pour opérer dans des espaces de caractéristiques comportementales qui sont non structurés (c'est-à-dire non discrétisés a priori en une grille) et non bornés.29 Cette capacité est particulièrement importante pour les domaines où les descripteurs comportementaux pertinents doivent être appris dynamiquement par l'algorithme ou sont intrinsèquement continus et difficiles à discrétiser. MOUR-QD a démontré sa supériorité par rapport aux méthodes MOQD basées sur une grille dans de tels scénarios, notamment dans des tâches robotiques où les caractéristiques doivent être apprises.  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Découvrir un large éventail de solutions qui non seulement optimisent plusieurs objectifs contradictoires mais qui présentent également une diversité significative en termes de comportements ou de caractéristiques structurelles. Ceci est particulièrement pertinent lorsque l'espace des comportements souhaitables est vaste, inconnu a priori, ou continu.  
    * **Limitations Conceptuelles :** La définition de l'espace des caractéristiques comportementales (les "behavioural descriptors") est une étape critique et souvent non triviale, dont la pertinence conditionne fortement la qualité des résultats. La scalabilité des algorithmes MOQD avec l'augmentation du nombre d'objectifs de performance et la dimension de l'espace des caractéristiques comportementales reste un défi de recherche actif. Les garanties théoriques concernant la couverture du front de Pareto global peuvent être plus faibles que celles des MOEAs classiques, car l'accent est mis sur la découverte de fronts de Pareto locaux au sein de chaque niche comportementale et sur la diversité de ces niches.  
  * **iv. Références Clés :**  
    1. Janmohamed, H., & Cully, A. (2025). Multi-Objective Quality-Diversity in Unstructured and Unbounded Spaces. *arXiv preprint arXiv:2504.03715*. 29  
    2. Pierrot, H., Flageat, M., Chalumeau, F., Sigaud, O., & Diaz-Rodriguez, N. (2022). Multi-objective quality-diversity. *Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)*, 819-827. 29 (Travail fondamental introduisant le concept de MOQD).  
    3. Cully, A., & Demiris, Y. (2018). Quality and diversity optimization: A unifying modular framework. *IEEE Transactions on Evolutionary Computation*, 22(2), 245-259. 29 (Article influent sur le cadre général QD).

##### **Métriques SOTA pour évaluer la progression et la qualité d'un ensemble de solutions de Pareto**

* **Approche 1: Indicateur d'Hypervolume (HV)**  
  * **i. Description Théorique :** L'indicateur d'Hypervolume (HV), également connu sous le nom de S-metric ou mesure de Lebesgue, est l'une des métriques les plus utilisées et les mieux fondées théoriquement pour évaluer la qualité d'un ensemble de solutions approchant un front de Pareto.28 Il mesure le volume (ou l'hypervolume en dimension M \> 3\) de la portion de l'espace objectif qui est dominée par l'ensemble de solutions considéré, par rapport à un point de référence prédéfini "le pire possible". L'HV a la propriété désirable d'être strictement conforme à la relation de dominance de Pareto : si un ensemble de solutions A domine un ensemble B, alors l'HV de A sera strictement supérieur à l'HV de B. Il capture ainsi à la fois la convergence des solutions vers le vrai front de Pareto et leur diversité le long de ce front.  
  * **ii. Avancées Récentes (Post-2022) :** Bien que le concept d'Hypervolume soit bien établi, la recherche active se poursuit, notamment sur le développement d'algorithmes plus efficaces pour son calcul exact ou son approximation, un défi qui devient particulièrement ardu lorsque le nombre d'objectifs (M) est élevé (typiquement M ≥ 3).28 Son utilisation comme critère de sélection directe ou comme guide au sein même des algorithmes MOEA continue d'être explorée et raffinée. De plus, la polyvalence de l'HV est démontrée par son adaptation à de nouveaux contextes, tels que la définition d'"hypervolume contradictoire" (adversarial hypervolume) pour quantifier la robustesse des modèles d'apprentissage profond face à une gamme d'intensités de perturbation, comme proposé par Liu et al. (arXiv 2024).34  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Fournir une mesure scalaire unique et agrégée de la qualité globale d'un ensemble de solutions de Pareto approximées. Cette mesure doit idéalement refléter à la fois la proximité de l'ensemble par rapport au vrai front de Pareto (convergence) et la bonne distribution des solutions le long de ce front (diversité).  
    * **Limitations Conceptuelles :** Le calcul exact de l'Hypervolume a une complexité computationnelle qui est exponentielle en fonction du nombre d'objectifs M pour M ≥ 3, ce qui le rend impraticable pour les problèmes avec un très grand nombre d'objectifs sans recourir à des approximations. Le choix du point de référence est crucial et peut influencer de manière significative la valeur absolue de l'HV ainsi que le classement relatif de différents ensembles de solutions. Un point de référence mal choisi peut masquer certaines qualités ou faiblesses d'un ensemble. L'HV peut également être parfois insensible à certaines améliorations de la diversité des solutions si ces améliorations n'entraînent pas une augmentation du volume dominé (par exemple, l'ajout de points dans des régions déjà bien couvertes mais qui améliorent l'uniformité).  
  * **iv. Références Clés :**  
    1. Zitzler, E., & Thiele, L. (1999). Multiobjective evolutionary algorithms: A comparative case study and the strength Pareto approach. *IEEE Transactions on Evolutionary Computation*, 3(4), 257-271. (Un des travaux pionniers et influents sur les métriques en MOO, y compris l'HV).  
    2. Beume, N., Naujoks, B., & Emmerich, M. (2007). SMS-EMOA: Multiobjective selection based on dominated hypervolume. *European Journal of Operational Research*, 181(3), 1653-1669. (Exemple d'un MOEA populaire qui utilise l'HV comme principal critère de sélection).  
    3. While, L., Bradstreet, L., & Barone, L. (2018). A fast way to calculate exact hypervolume. *IEEE Transactions on Evolutionary Computation*, 22(5), 799-810. (Exemple de recherche sur l'efficacité du calcul de l'HV).  
* **Approche 2: Métriques de Qualité-Diversité Multi-Objectifs (spécifiques aux algorithmes MOQD)**  
  * **i. Description Théorique :** Pour les algorithmes de Qualité-Diversité Multi-Objectifs (MOQD), qui visent simultanément l'optimalité par rapport à plusieurs objectifs et la diversité comportementale, des métriques spécifiques ont été développées pour évaluer ces deux aspects de manière conjointe.  
  * **ii. Avancées Récentes (Post-2022) :** Dans le cadre de leurs travaux sur MOUR-QD, Janmohamed et Cully (arXiv 2025\) utilisent ou proposent un ensemble de métriques adaptées 29 :  
    1. **moqd-score**: Cette métrique quantifie la performance globale des solutions stockées dans l'archive (le répertoire des solutions). Elle est calculée comme la somme des hypervolumes des fronts de Pareto locaux trouvés au sein de chaque cellule d'une grille de caractéristiques (si une grille est utilisée) ou de chaque niche comportementale identifiée (dans le cas d'un répertoire non structuré). Le moqd-score capture ainsi à la fois la performance locale des solutions par rapport aux objectifs et la diversité globale des comportements couverts par l'archive.  
    2. **global-hypervolume**: Il s'agit de l'hypervolume du front de Pareto global formé par l'ensemble de toutes les solutions présentes dans l'archive, indépendamment de leur assignation à une niche comportementale ou de la structure de l'espace des caractéristiques. Cette métrique évalue les meilleurs compromis globaux découverts par l'algorithme sur l'ensemble des objectifs.  
    3. **coverage**: Cette métrique simple mesure la fraction des cellules de l'archive (ou des régions discrétisées de l'espace des caractéristiques) qui contiennent au moins une solution. Elle reflète directement la diversité des solutions en termes de couverture de l'espace comportemental exploré.  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Évaluer de manière comprehensive la performance des algorithmes MOQD, qui ont le double objectif d'atteindre l'optimalité au sens de Pareto et de maximiser la diversité comportementale.  
    * **Limitations Conceptuelles :** Le moqd-score et la coverage sont sensibles à la manière dont l'espace des caractéristiques est défini et, le cas échéant, discrétisé (par exemple, la granularité d'une grille). Des choix différents de discrétisation peuvent conduire à des valeurs de métriques différentes et potentiellement à des classements d'algorithmes différents. Ces métriques sont intrinsèquement liées aux objectifs spécifiques des algorithmes QD et peuvent ne pas être directement comparables aux métriques MOO standard si la diversité comportementale n'est pas un objectif explicite du problème ou de l'algorithme évalué.  
  * **iv. Références Clés :**  
    1. Janmohamed, H., & Cully, A. (2025). Multi-Objective Quality-Diversity in Unstructured and Unbounded Spaces. *arXiv preprint arXiv:2504.03715*. 29  
    2. Cully, A., & Demiris, Y. (2018). Quality and diversity optimization: A unifying modular framework. *IEEE Transactions on Evolutionary Computation*, 22(2), 245-259. 29 (Article fondateur pour le qd-score en mono-objectif, dont le moqd-score est une extension).  
    3. Pugh, J. K., Soros, L. B., & Stanley, K. O. (2016). Quality diversity: A new frontier for evolutionary computation. *Frontiers in Robotics and AI*, 3, 40\. (Vue d'ensemble du paradigme QD).

Le caractère itératif du modèle PI-QPM, avec son "Exploration Itérative à Gain Décroissant", implique que l'évaluation de la "bonté" d'un plan, ou plus généralement d'un ensemble de plans candidats non dominés, doit pouvoir s'effectuer de manière continue et progressive. À chaque itération de la boucle de "réflexion" du PI-QPM, de nouvelles informations peuvent être acquises, conduisant potentiellement à la génération de nouveaux plans partiels ou à la mise à jour de plans existants. Le "Jugement Objectif" doit alors évaluer ces plans. Si plusieurs plans non dominés coexistent, ils forment un front de Pareto partiel qui évolue avec l'exploration. Le système doit ensuite décider si une exploration supplémentaire est justifiée, c'est-à-dire si le "gain" attendu est suffisant. Cette décision pourrait être informée par l'amélioration observée du front de Pareto, par exemple mesurée par l'augmentation de son hypervolume. L'article de de Nobel et al. sur le benchmarking des MOO 28 met justement en exergue que, bien que "l'analyse anytime des optimiseurs multi-objectifs soit plus difficile", elle est cruciale pour comprendre leur comportement. Ils préconisent l'utilisation d'un "archivage externe non borné" de toutes les solutions non dominées évaluées, ce qui permet une analyse flexible et l'application monotone d'indicateurs Pareto-conformes comme l'hypervolume. Par conséquent, le système PI-QPM devrait intégrer des mécanismes d'archivage des solutions non dominées découvertes au fil des itérations et calculer des métriques telles que l'hypervolume de manière incrémentale. Ces métriques pourraient alors servir de base quantitative pour guider la boucle de "réflexion" et pour actionner le critère d'arrêt lié au "Gain Décroissant".

Le tableau suivant offre une synthèse des approches SOTA pour ce pilier.

**Tableau 3.1: Synopsis des Approches SOTA pour la Théorie de la Décision Multi-Objectifs**

| Approche/Algorithme/Métrique | Principe Théorique | Avancées Récentes (Post-2022) | Problème Ciblé | Limitations Conceptuelles | Références Clés |
| :---- | :---- | :---- | :---- | :---- | :---- |
| MOEAs (NSGA-III/NG, MOEA/D-NG) | Algorithmes évolutionnaires basés sur la décomposition (MOEA/D) ou les points de référence (NSGA-III) pour approximer le front de Pareto. | Nouvelle stratégie de recherche "NG" (Gao et al., 2024\) améliorant convergence et précision..27 Benchmarking "anytime".28 | Trouver un ensemble diversifié de solutions Pareto-optimales pour MOO complexes, y compris many-objective. | Dépendance aux paramètres (points de réf./poids). Sensibilité à la forme du front. Garanties probabilistes. Gestion des dynamiques. | Gao et al. (2024) 27, Deb & Jain (2014), Zhang & Li (2007) 26 |
| MOQD (MOUR-QD) | Algorithmes de Qualité-Diversité pour MOO, cherchant des fronts de Pareto locaux dans des niches comportementales diverses. | MOUR-QD (Janmohamed & Cully, arXiv 2025\) pour espaces de caractéristiques non structurés/non bornés. 29 | Découvrir des solutions Pareto-optimales avec une grande diversité comportementale, surtout si l'espace des caractéristiques est inconnu/continu. | Définition des descripteurs comportementaux. Scalabilité (nb objectifs, dim. caractéristiques). Garanties sur le front global plus faibles. | Janmohamed & Cully (arXiv 2025\) 29, Pierrot et al. (GECCO 2022\) 29 |
| Indicateur d'Hypervolume (HV) | Mesure le volume de l'espace objectif dominé par un ensemble de solutions par rapport à un point de référence. Strictement Pareto-conforme. | Algorithmes de calcul plus efficaces. Application à de nouveaux contextes (ex: adversarial hypervolume 34). | Mesure scalaire unique de la qualité (convergence et diversité) d'un front de Pareto. | Calcul exact coûteux (M ≥ 3). Sensibilité au point de référence. Peut être insensible à certaines améliorations de diversité. | Zitzler & Thiele (1999), Beume et al. (2007) |
| Métriques MOQD (moqd-score, global-hypervolume, coverage) | Métriques pour évaluer la performance des algorithmes MOQD. | moqd-score (somme des HV locaux), global-hypervolume (HV de toutes solutions), coverage (fraction de niches remplies) (Janmohamed & Cully, arXiv 2025). 29 | Évaluer la performance (qualité et diversité comportementale) des algorithmes MOQD. | Dépendance à la discrétisation/définition des niches. Spécifiques aux objectifs QD. | Janmohamed & Cully (arXiv 2025\) 29, Cully & Demiris (2018) 29 |

### **Pilier 4 : Représentation des Connaissances et Raisonnement Probabiliste**

#### **2.4.a. Introduction au problème théorique du pilier**

Le "Graphe de Potentiel Limité" au cœur du modèle PI-QPM exige un formalisme de représentation des connaissances capable de capturer simultanément la structure des "chemins d'exécution" potentiels – incluant des faits établis et des hypothèses – et l'incertitude inhérente associée à ces éléments, exprimée sous forme de probabilités. Ce pilier se consacre à l'exploration des formalismes état de l'art pour de tels Graphes de Connaissances Probabilistes (Probabilistic Knowledge Graphs \- PKGs). Il s'intéresse également aux algorithmes d'inférence, tels que la Propagation de Croyance (Belief Propagation) et d'autres méthodes d'inférence bayésienne, qui permettent de raisonner sur ces structures complexes et incertaines. Une fonction clé de ces algorithmes d'inférence dans le contexte de PI-QPM est de permettre la "Mise à Jour Probabiliste" des croyances du système concernant la validité des hypothèses et la probabilité des chemins d'exécution, suite à l'observation des résultats des actions entreprises.

#### **2.4.b. Présentation des approches/algorithmes SOTA les plus pertinents**

##### **Formalismes SOTA pour les Graphes de Connaissances Probabilistes (PKGs) ou les modèles combinant logique du premier ordre et raisonnement probabiliste**

* **Approche 1: Modèles d'Embeddings de KGs Probabilistes (Probabilistic Knowledge Graph Embeddings \- PKGE)**  
  * **i. Description Théorique :** Les modèles d'embeddings de KGs visent à apprendre des représentations vectorielles denses (embeddings) pour les entités et les relations au sein d'un graphe de connaissances. Ces embeddings capturent les propriétés sémantiques et structurelles du KG. Les PKGE étendent cette idée en intégrant explicitement l'incertitude. Certaines approches y parviennent en modélisant les entités et/ou les relations non pas comme des points uniques dans un espace vectoriel, mais comme des distributions de probabilité (par exemple, des Gaussiennes) sur cet espace. D'autres approches, comme UKGE (Uncertain Knowledge Graph Embedding) 35, opèrent sur des KGs où les faits (triplets sujet-prédicat-objet) sont annotés avec des scores de confiance ou des probabilités. L'objectif est alors d'apprendre des embeddings qui peuvent prédire non seulement l'existence d'un lien, mais aussi son degré de certitude.  
  * **ii. Avancées Récentes (Post-2022) :** La recherche sur les PKGE est active. La proposition de thèse de Zhu (NAACL-SRW 2025\) 35 fait référence à des travaux récents (par exemple, Zhu et al., 2023, 2024c) qui continuent de développer des méthodes associant des scores de confiance ou des probabilités aux faits et apprenant des embeddings qui incorporent à la fois la structure du KG et l'incertitude des données d'entrée. Des améliorations aux modèles existants comme UKGE ont été proposées, notamment par Chen et al. (2021a,b), qui ont introduit de meilleures stratégies d'échantillonnage négatif et des représentations géométriques plus expressives pour les entités (sous forme de "boîtes" ou hyper-rectangles) et les relations (sous forme de transformations affines) afin de mieux capturer l'incertitude et les hiérarchies.35 Par ailleurs, des travaux émergents explorent l'utilisation de Grands Modèles de Langage (LLM) pour la génération de règles, y compris potentiellement des règles probabilistes, sur des KGs, comme l'approche LLM-DR pour les KGs temporels (Chen et al., AAAI 2025\) 36, ce qui pourrait ouvrir de nouvelles voies pour enrichir les PKGs.  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Apprendre des représentations latentes de graphes de connaissances qui capturent et quantifient l'incertitude associée aux faits et aux relations. Ces représentations sont ensuite utilisées pour des tâches telles que la complétion de KG (prédire des liens manquants avec leur probabilité), la prédiction de liens, ou le question-réponse sous incertitude.  
    * **Limitations Conceptuelles :** Une limitation majeure est que la qualité de l'incertitude modélisée par les embeddings (par exemple, sa calibration probabiliste) n'est pas toujours systématiquement étudiée ou garantie.35 De nombreux modèles d'embedding probabilistes, en particulier ceux qui représentent les entités/relations comme des distributions complexes, peuvent nécessiter un nombre de paramètres significativement plus élevé et engendrer des coûts de calcul importants lors de l'apprentissage et de l'inférence, en raison de la nécessité de calculer des distances ou des scores entre distributions.35 L'interprétabilité des embeddings eux-mêmes, et plus encore de l'incertitude qu'ils capturent, reste un défi conceptuel et pratique.  
  * **iv. Références Clés :**  
    1. Chen, X., et al. (2019). Embedding Uncertain Knowledge Graphs. *Proceedings of the AAAI Conference on Artificial Intelligence*, 33(01), 3238-3245. (Article fondateur d'UKGE) 35  
    2. Zhu, Q. (2025). Thesis Proposal: Uncertainty in Knowledge Graph Embeddings. *Proceedings of the NAACL Student Research Workshop*. 35  
    3. Nickel, M., Murphy, K., Tresp, V., & Gabrilovich, E. (2016). A review of relational machine learning for knowledge graphs. *Proceedings of the IEEE*, 104(1), 11-33. (Revue générale pertinente, bien qu'antérieure à 2022, elle pose les bases).  
* **Approche 2: Logique Souple Probabiliste (Probabilistic Soft Logic \- PSL)**  
  * **i. Description Théorique :** La Logique Souple Probabiliste (PSL) est un cadre formel et un langage de modélisation pour l'apprentissage statistique relationnel (Statistical Relational Learning \- SRL).37 PSL combine l'expressivité de la logique du premier ordre avec la capacité des modèles graphiques probabilistes à gérer l'incertitude. Contrairement à la logique classique où les prédicats sont soit vrais soit faux, PSL utilise des valeurs de vérité "souples" (soft truth values) dans l'intervalle continu pour les atomes. Un programme PSL est constitué d'un ensemble de règles logiques (similaires à des clauses de Horn, mais avec des implications souples basées sur la logique de Łukasiewicz) auxquelles sont associés des poids. Ces règles pondérées définissent la structure d'un type particulier de champ aléatoire de Markov appelé Hinge-Loss Markov Random Field (HL-MRF). Une caractéristique clé de PSL est que l'inférence, c'est-à-dire la tâche de trouver l'assignation de valeurs de vérité la plus probable aux atomes non observés (Maximum A Posteriori \- MAP inference), peut être formulée comme un problème d'optimisation convexe. Cela rend l'inférence en PSL typiquement plus rapide et plus scalable que dans d'autres formalismes SRL comme les Réseaux Logiques de Markov (Markov Logic Networks \- MLNs), où l'inférence est \#P-complète.37  
  * **ii. Avancées Récentes (Post-2022) :** Bien que PSL soit un formalisme relativement mature (sa version stable 2.2.2 date de mai 2020 37), son application continue dans divers domaines et son intégration potentielle avec des graphes de connaissances pour le raisonnement sous incertitude demeurent pertinentes. Les recherches récentes impliquant PSL peuvent se concentrer davantage sur des aspects tels que l'apprentissage efficace des poids des règles à partir de données, l'extension du langage de règles pour une plus grande expressivité, l'amélioration de la scalabilité des algorithmes d'inférence et d'apprentissage pour des KGs de très grande taille, ou encore son application à de nouveaux types de problèmes. Peu de sources identifiées mentionnent des avancées théoriques fondamentales *au sein* du formalisme PSL lui-même après 2022, mais son statut en tant qu'approche SOTA pour combiner logique et probabilités avec une inférence tractable persiste.  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Modélisation de domaines qui présentent à la fois une structure relationnelle complexe (capturable par la logique) et une incertitude inhérente (capturable par les probabilités). Les applications typiques incluent l'inférence collective (où la valeur d'un prédicat pour une entité dépend des valeurs pour d'autres entités reliées), la résolution d'entités, la prédiction de liens, et l'alignement d'ontologies.  
    * **Limitations Conceptuelles :** L'expressivité de PSL est contrainte par l'utilisation des connecteurs logiques spécifiques de la t-norme de Łukasiewicz et par la forme des règles autorisées. L'interprétation des valeurs de vérité souples, bien que mathématiquement bien définies, peut parfois être moins intuitive pour les utilisateurs que des probabilités directes attachées à des faits. L'apprentissage des poids des règles à partir de données peut être un processus complexe et sensible à la qualité et à la quantité des données d'entraînement disponibles. La conversion de connaissances expertes en règles PSL pondérées peut également s'avérer non triviale.  
  * **iv. Références Clés :**  
    1. Bach, S. H., Broecheler, M., Getoor, L., & O'Connor, A. (2017). Hinge-Loss Markov Random Fields and Probabilistic Soft Logic. *Journal of Machine Learning Research*, 18, 1-67. 37 (Article fondamental détaillant la théorie de PSL et des HL-MRFs).  
    2. Kimmig, A., Bach, S. H., Broecheler, M., Getoor, L., & Kok, S. (2012). A short introduction to Probabilistic Soft Logic. *Proceedings of the NIPS Workshop on Probabilistic Programming: Foundations and Applications*. (Une introduction concise et accessible à PSL).  
    3. Getoor, L., & Taskar, B. (Eds.). (2007). *Introduction to statistical relational learning*. MIT press. (Ouvrage de référence sur le SRL, fournissant le contexte plus large).

##### **Algorithmes SOTA pour la Propagation de Croyance (Belief Propagation \- BP) ou l'inférence bayésienne sur des structures graphiques de grande taille et potentiellement cycliques**

* **Approche 1: Variantes de Belief Propagation avec Garanties de Convergence ou Robustesse Améliorée**  
  * **i. Description Théorique :** La Propagation de Croyance (Belief Propagation \- BP), également connue sous le nom d'algorithme somme-produit, est un algorithme de passage de messages conçu pour effectuer l'inférence (typiquement, calculer les probabilités marginales des variables) sur des modèles graphiques probabilistes tels que les réseaux bayésiens ou les champs aléatoires de Markov.39 Lorsque le graphe sous-jacent est un arbre (acyclique), BP converge vers les marginaux exacts en un temps proportionnel à la taille du graphe. Cependant, pour les graphes contenant des cycles (communément appelés "loopy graphs"), l'application de BP (alors appelée "Loopy BP") n'est plus garantie de converger, et même si elle converge, les marginaux obtenus ne sont qu'approximatifs.39 Malgré cela, Loopy BP a démontré un succès empirique surprenant dans de nombreuses applications.  
  * **ii. Avancées Récentes (Post-2022) :**  
    1. **Méthodes pour Améliorer la Convergence et la Précision de Loopy BP :** Bien que les fondements de ces méthodes soient souvent antérieurs à 2022, leur pertinence pour adresser les défis actuels de Loopy BP demeure. Par exemple, la Convex Combination Belief Propagation (CCBP), décrite dans la thèse de Grim (2018) 41, modifie l'opérateur de mise à jour des messages de Loopy BP de manière à ce qu'il devienne une contraction, garantissant ainsi la convergence vers un point fixe unique, quelle que soit la topologie du graphe (y compris les cycles). Pour améliorer la précision, qui peut être moindre avec CCBP qu'avec Loopy BP (si ce dernier converge), des méthodes d'homotopie peuvent être employées pour déformer progressivement l'opérateur de CCBP vers celui de Loopy BP, cherchant à hériter de la convergence de l'un et de la précision potentielle de l'autre.41  
    2. **Analyse Approfondie du Comportement de Loopy BP :** Des travaux récents, comme ceux de Nagpal et al. (IACR Communications in Cryptology, 2025, bien que la soumission date de 2024 42), se sont penchés sur une analyse systématique du comportement de Loopy BP dans des contextes spécifiques, en le comparant à l'inférence exacte et en évaluant l'impact de diverses heuristiques proposées pour améliorer ses performances.40 Leurs conclusions, bien que dans le domaine des attaques par canal auxiliaire, soulignent qu'il existe souvent un écart significatif entre les résultats de Loopy BP et ceux de l'inférence exacte, et que la plupart des heuristiques courantes ont un impact globalement minimal sur la réduction de cet écart.40  
    3. **Generalized Belief Propagation (GBP) et Approximations de Kikuchi :** Ces approches, dont les bases ont été posées par Yedidia et al. (autour de 2000-2005) 43, offrent un cadre théorique pour construire des approximations de l'énergie libre plus précises que l'approximation de Bethe (à laquelle Loopy BP est connectée). Les algorithmes GBP dérivés de ces approximations de Kikuchi d'ordre supérieur impliquent un passage de messages entre des "régions" (clusters de variables et de facteurs) plus grandes que les simples paires de variables. Ils peuvent offrir une meilleure précision et parfois converger lorsque Loopy BP ordinaire échoue, mais au prix d'une complexité computationnelle accrue.43  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Effectuer une inférence marginale approximative sur des modèles graphiques probabilistes de grande taille qui contiennent des cycles, situations où l'inférence exacte est généralement intraitable.  
    * **Limitations Conceptuelles :** Pour **Loopy BP** standard, l'absence de garantie de convergence générale est une limitation majeure. Même en cas de convergence, il n'y a pas de garantie sur l'exactitude des marginaux calculés, et la qualité de l'approximation dépend fortement de la structure spécifique du graphe et des paramètres du modèle. **CCBP** garantit la convergence mais peut sacrifier une partie de la précision potentielle de Loopy BP. Les méthodes **GBP/Kikuchi** d'ordre supérieur peuvent être plus précises mais leur complexité computationnelle augmente avec la taille des régions choisies, et le choix optimal de ces régions est lui-même un problème difficile. La plupart des variantes de BP, y compris les plus sophistiquées, peuvent avoir des difficultés à capturer des dépendances à longue portée dans les graphes très denses ou avec des structures cycliques complexes.  
  * **iv. Références Clés :**  
    1. Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2005). Constructing free-energy approximations and generalized belief propagation algorithms. *IEEE Transactions on Information Theory*, 51(7), 2282-2312. 43 (Article de référence sur GBP et les connexions avec les approximations de Bethe et Kikuchi).  
    2. Grim, B. (2018). *Message Passing Dynamics of Belief Propagation Algorithms*. PhD Thesis, Brown University. 41 (Pour CCBP et les méthodes d'homotopie).  
    3. Nagpal, R., Cassiers, G., Primas, R., Knoll, C., Pernkopf, F., & Mangard, S. (2025). On Loopy Belief Propagation for SASCAs. *IACR Communications in Cryptology*, 1(1), 15\. 40 (Pour une analyse récente du comportement de Loopy BP).  
* **Approche 2: Inférence Variationnelle (VI) Adaptative**  
  * **i. Description Théorique :** L'Inférence Variationnelle (VI) est une famille de techniques qui transforment le problème de l'inférence bayésienne (calcul d'une distribution a posteriori P(Z∣X)) en un problème d'optimisation.44 L'idée est de choisir une famille de distributions Q(Z) paramétrée (appelée famille variationnelle), qui est supposée être plus simple à manipuler que la vraie postérieure, puis de trouver le membre de cette famille qui est le "plus proche" de la vraie postérieure, généralement en minimisant une mesure de divergence comme la divergence de Kullback-Leibler (KL) KL(Q∣∣P). Minimiser KL(Q∣∣P) est équivalent à maximiser une borne inférieure de l'évidence (Evidence Lower Bound \- ELBO). Des approximations classiques comme l'approximation de Bethe et les énergies libres tree-reweighted peuvent être vues comme des cas particuliers ou des inspirations pour certaines formes d'inférence variationnelle.44  
  * **ii. Avancées Récentes (Post-2022) :** Leisenberger et Pernkopf (soumission à UAI 2025\) proposent des approches d'inférence variationnelle qui s'adaptent automatiquement au modèle probabiliste graphique donné.44 Ils analysent les limitations des approximations variationnelles standard (comme Bethe, tree-reweighted, et d'autres énergies libres convexes) qui, bien qu'efficaces dans de nombreux cas, peuvent échouer ou fournir des approximations de mauvaise qualité lorsque le modèle sous-jacent est complexe, avec de fortes interactions entre variables. Leur travail explore deux directions principales pour améliorer ces approximations : soit en modifiant les paramètres du modèle (l'énergie de l'état), soit en modifiant l'approximation de l'entropie utilisée dans la formulation de l'énergie libre. Sur la base de cette analyse, ils proposent des approximations qui s'ajustent dynamiquement à la structure du modèle, démontrant leur efficacité sur des problèmes d'inférence difficiles.44  
  * **iii. Problème Ciblé et Limitations Conceptuelles :**  
    * **Problème Ciblé :** Fournir une inférence approximative qui soit à la fois robuste et précise pour des modèles graphiques probabilistes complexes, en particulier lorsque les méthodes variationnelles standard (par exemple, mean-field ou Bethe) s'avèrent insuffisantes en raison de la complexité des dépendances ou des interactions dans le modèle.  
    * **Limitations Conceptuelles :** La principale limitation de toute approche VI réside dans le choix de la famille variationnelle Q. Si la vraie postérieure est très différente de toutes les distributions de la famille Q choisie, alors l'approximation sera nécessairement de mauvaise qualité, quelle que soit la précision de l'optimisation. L'optimisation elle-même peut être non convexe et difficile, conduisant potentiellement à des optima locaux plutôt qu'à la meilleure approximation globale au sein de la famille Q. Les méthodes adaptatives, bien que prometteuses, peuvent introduire une complexité supplémentaire dans la formulation du problème variationnel ou dans le processus d'optimisation. Il est bien connu que certaines formes simples de VI, comme l'approximation mean-field (où Q est entièrement factorisée), ont tendance à sous-estimer la variance de la distribution a posteriori.46  
  * **iv. Références Clés :**  
    1. Leisenberger, H., & Pernkopf, F. (2025). Adaptive Variational Inference in Probabilistic Graphical Models: Beyond Bethe, Tree-Reweighted, and Convex Free Energies. *arXiv preprint arXiv:2502.03341*. (Soumis à UAI 2025\) 44  
    2. Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational Inference: A Review for Statisticians. *Journal of the American Statistical Association*, 112(518), 859-877. (Une revue fondamentale et accessible de l'inférence variationnelle).  
    3. Zhang, C., Bütepage, J., Kjellström, H., & Mandt, S. (2018). Advances in variational inference. *IEEE transactions on pattern analysis and machine intelligence*, 41(8), 2008-2026. (Une revue plus récente couvrant des avancées).

Le "Graphe de Potentiel Limité" du modèle PI-QPM est non seulement de nature probabiliste, mais il est également conçu pour être dynamique : les croyances associées à ses nœuds et arêtes (représentant des faits, hypothèses, et chemins d'exécution) sont sujettes à une "Mise à Jour Probabiliste" continue, alimentée par les résultats des actions entreprises par le système. Cette nature dynamique et incertaine exige des méthodes d'inférence (telles que la Propagation de Croyance ou l'Inférence Variationnelle) qui possèdent plusieurs qualités essentielles. Premièrement, elles doivent être capables de gérer des structures graphiques de grande taille et potentiellement cycliques, ce qui est typique des graphes de connaissances réels. Deuxièmement, elles doivent être suffisamment efficaces pour permettre des mises à jour fréquentes sans induire une latence prohibitive dans la boucle de décision du PI-QPM. Troisièmement, elles doivent être robustes face à des informations qui peuvent être potentiellement contradictoires, bruitées, ou incomplètes. Les analyses récentes de Loopy BP 40 soulignent ses limitations en termes de garanties de convergence et de précision sur de tels graphes, et indiquent que les heuristiques courantes n'offrent souvent que des améliorations marginales. Des alternatives comme CCBP 41 offrent la convergence au prix d'une potentielle perte de précision, tandis que GBP 43 peut être plus précis mais est plus coûteux. L'Inférence Variationnelle 44, en particulier les approches adaptatives récentes, constitue une autre voie prometteuse, bien qu'elle vienne avec son propre ensemble de défis liés au choix de la famille variationnelle et à la complexité de l'optimisation. Ainsi, le PI-QPM devra soigneusement sélectionner ou potentiellement développer une approche d'inférence qui réalise un compromis judicieux entre précision, scalabilité pour des mises à jour fréquentes, et robustesse. La caractéristique "limitée" du graphe de potentiel pourrait également être exploitée pour focaliser les efforts d'inférence sur les parties les plus pertinentes du graphe à un instant donné.

Le tableau ci-dessous résume les approches SOTA pour ce pilier.

**Tableau 4.1: Synopsis des Approches SOTA pour la Représentation des Connaissances et le Raisonnement Probabiliste**

| Approche/Algorithme | Principe Théorique | Avancées Récentes (Post-2022) | Problème Ciblé | Limitations Conceptuelles | Références Clés |
| :---- | :---- | :---- | :---- | :---- | :---- |
| Embeddings de KGs Probabilistes (PKGE) | Apprentissage de représentations vectorielles (embeddings) pour entités/relations intégrant l'incertitude (distributions, scores de confiance). | Améliorations d'UKGE (Chen et al. 2021a,b). Travaux de Zhu et al. (2023, 2024c) sur l'intégration structure/incertitude..35 Exploration de LLM pour règles sur KG (Chen et al., AAAI 2025).36 | Apprentissage de représentations latentes de KGs incertains pour complétion, prédiction de liens, QA. | Qualité/calibration de l'incertitude modélisée. Coût calculatoire et nb de paramètres. Interprétabilité. | Chen et al. (AAAI 2019\) 35, Zhu (NAACL-SRW 2025\) 35 |
| Logique Souple Probabiliste (PSL) | Combine logique du premier ordre (valeurs de vérité souples ) et modèles graphiques (Hinge-Loss MRFs). Inférence MAP par optimisation convexe. | Applications continues et intégrations potentielles. Recherche sur apprentissage des poids, scalabilité. (Peu d'avancées théoriques fondamentales SOTA post-2022 sur le formalisme lui-même). 37 | Modélisation de domaines probabilistes et relationnels, inférence collective, résolution d'entités, prédiction de liens. | Expressivité limitée par connecteurs/forme des règles. Interprétation des valeurs souples. Apprentissage des poids. | Bach et al. (JMLR 2017\) 37, Kimmig et al. (NIPS WS 2012\) |
| Variantes de Belief Propagation (BP) | Passage de messages pour inférence marginale. Loopy BP pour graphes cycliques (approximatif, convergence non garantie). | CCBP pour convergence garantie (Grim 2018).41 Analyse de Loopy BP et heuristiques (Nagpal et al., IACR CoC 2025).40 GBP/Kikuchi pour meilleure précision (coûteux).43 | Inférence marginale approximative sur grands modèles graphiques cycliques. | Loopy BP: pas de garantie de convergence/exactitude. CCBP: convergence mais peut être moins précis. GBP: complexité, choix des régions. Dépendances longues. | Yedidia et al. (IEEE TIT 2005\) 43, Grim (PhD Thesis 2018\) 41, Nagpal et al. (IACR CoC 2025\) 40 |
| Inférence Variationnelle (VI) Adaptative | Reformule l'inférence comme optimisation (minimiser divergence KL entre Q et P). | VI adaptative (Leisenberger & Pernkopf, sub. UAI 2025\) s'ajustant au modèle, généralisant l'approx. d'entropie ou l'énergie d'état. 44 | Inférence approximative robuste et précise pour modèles graphiques complexes où VI standard échoue. | Choix de la famille variationnelle Q. Optimisation non convexe (optima locaux). Complexité des méthodes adaptatives. Sous-estimation de la variance. | Leisenberger & Pernkopf (arXiv 2025\) 44, Blei et al. (JASA 2017\) |

## **3\. Conclusion : Synthèse des convergences ou des tensions entre les différentes approches algorithmiques pour construire un système unifié comme PI-QPM**

L'analyse des approches algorithmiques état de l'art pertinentes pour les quatre piliers conceptuels du modèle PI-QPM révèle des convergences prometteuses ainsi que des tensions et des défis de recherche ouverts, qui seront cruciaux à considérer pour la construction d'un système unifié.

**Convergences Notables :**

* Une tendance forte se dégage vers une **intégration de plus en plus étroite entre l'apprentissage machine et les mécanismes de raisonnement et de planification plus classiques**. Des exemples incluent les approches DRL pour les POMDPs comme UniZero 15 qui apprennent un modèle du monde utilisé ensuite pour la planification par MCTS, l'apprentissage de méthodes de décomposition HTN à partir d'observations 2, ou encore l'apprentissage de modèles de substitution dans l'Optimisation Bayésienne.22 Le modèle PI-QPM, avec sa boucle d'exploration itérative et sa mise à jour probabiliste continue de son graphe de connaissances, s'inscrit naturellement dans cette mouvance, suggérant un système qui apprend et s'adapte en continu.  
* La **gestion explicite et rigoureuse de l'incertitude** est une autre convergence majeure. Les formalismes tels que les POMDPs 8, les Graphes de Connaissances Probabilistes (qu'ils soient basés sur des embeddings 35 ou des logiques probabilistes comme PSL 37), et les stratégies d'exploration explicitement guidées par le gain d'information (comme dans les ρPOMDPs 10, l'Optimisation Bayésienne 22, ou certaines formes d'Apprentissage par Renforcement information-théorique 24) placent la modélisation, la quantification et la réduction de l'incertitude au cœur même du processus décisionnel. Ceci est fondamental pour PI-QPM, qui opère sur des "hypothèses et probabilités" et vise une "exécution par point de levier" pour maximiser le gain d'information.  
* Face à la complexité inhérente aux problèmes du monde réel, de nombreuses approches SOTA s'appuient sur des **approximations intelligentes et des mécanismes d'abstraction pour atteindre la scalabilité**. Cela se manifeste dans l'utilisation de l'échantillonnage pour les solveurs POMDP en ligne 47, la nature heuristique de MCTS 20, les fondements de l'Inférence Variationnelle 44 et de Loopy Belief Propagation 40, ainsi que dans la nature compressive des embeddings de graphes de connaissances.35 Les Réseaux de Tâches Hiérarchiques 1 offrent une forme d'abstraction structurelle, et même les garanties déterministes pour POMDPs proposées par Lim et al. 8 s'appuient sur des POMDPs simplifiés. Le concept même de "Graphe de Potentiel Limité" au sein de PI-QPM suggère une telle stratégie de focalisation des ressources computationnelles sur les aspects les plus prometteurs ou incertains du problème.

**Tensions et Défis Ouverts :**

* Une tension significative existe entre la nécessité d'une **gestion robuste de l'observabilité partielle et l'intégration d'une structure hiérarchique explicite**. Les formalismes HTN, très utiles pour la décomposition et la structuration des plans, supposent souvent une forte observabilité (comme les HTN FOND 1) ou un non-déterminisme limité aux actions. Inversement, les POMDPs, qui sont le cadre de choix pour l'observabilité partielle, n'intègrent pas nativement des mécanismes de décomposition hiérarchique aussi riches et sémantiquement expressifs que les méthodes HTN. Combler ce fossé est un défi majeur pour un système comme PI-QPM qui semble requérir les deux.  
* L'**expressivité des formalismes de Graphes de Connaissances Probabilistes (PKG) doit être mise en balance avec la tractabilité de l'inférence**. Des PKGs plus expressifs, capables par exemple de combiner une logique du premier ordre riche avec des modèles probabilistes sophistiqués, peuvent rendre l'inférence (nécessaire pour la "Mise à Jour Probabiliste" de PI-QPM) intraitable ou extrêmement coûteuse, surtout si des mises à jour fréquentes sont requises comme le suggère la nature itérative du système.  
* La **quantification du "gain d'information" de manière pertinente pour les objectifs globaux du projet** géré par PI-QPM est un défi conceptuel. Les différentes approches pour maximiser le gain d'information (issues de la BO, du RL information-théorique, ou des récompenses dans les ρPOMDPs) opèrent souvent au niveau de l'amélioration d'un modèle interne du monde, de la réduction de l'incertitude sur une politique, ou de la clarté de l'état de croyance. Traduire cela en un gain d'information qui reflète véritablement la progression vers les objectifs finaux du *projet* (et pas seulement l'amélioration d'un modèle interne de l'agent) nécessitera une réflexion approfondie.  
* L'**intégration effective du jugement multi-objectifs (Pilier 3\) au sein de la boucle de planification sous incertitude (Pilier 1\) et des stratégies d'exploration (Pilier 2\)** reste une question ouverte. Comment les évaluations du front de Pareto, issues de l'Optimisation Multi-Objectifs, influencent-elles dynamiquement les décisions de planification et d'exploration? Par exemple, le critère d'arrêt de "l'Exploration Itérative à Gain Décroissant" de PI-QPM pourrait-il être basé sur la stabilisation du front de Pareto, ou sur une amélioration de l'hypervolume qui devient marginale?

En considérant l'ensemble des défis et des capacités algorithmiques discutées, le système PI-QPM lui-même peut être envisagé comme un **méta-problème de décision séquentielle, opéré sous incertitude et visant des objectifs multiples**. L'agent PI-QPM doit prendre une séquence de décisions complexes : quelles hypothèses explorer plus avant dans son graphe de potentiel, quelle action exécuter comme "point de levier" pour maximiser l'impact, quand considérer que la "réflexion" (planification et exploration) a atteint un point de rendement décroissant et qu'il est temps d'agir ou de reconsidérer les stratégies globales. Ces décisions sont prises sous une incertitude omniprésente : incertitude sur l'état réel du projet, sur les résultats futurs des actions envisagées, sur la validité des hypothèses stockées dans le graphe de connaissances. Et ces décisions doivent viser à optimiser simultanément plusieurs objectifs potentiellement conflictuels (coût, risque, valeur, délai, etc.).

Les Piliers 1 (Planification et Décomposition sous Incertitude) et 4 (Représentation des Connaissances et Raisonnement Probabiliste) fournissent les outils pour modéliser l'état du monde et les actions possibles, ainsi que pour raisonner sur l'incertitude. Le Pilier 2 (Stratégies d'Exploration et de Sélection d'Actions) offre les mécanismes pour décider quelles informations rechercher ou quelles actions tester. Le Pilier 3 (Théorie de la Décision Multi-Objectifs) fournit les moyens d'évaluer la qualité des plans ou des stratégies envisagées par rapport à des critères multiples.

Le véritable défi et la promesse du PI-QPM résident dans l'orchestration cohérente et synergique de ces capacités. Par exemple, un solveur POMDP (issu du Pilier 1\) pourrait être utilisé pour la sélection du "point de levier". La fonction de récompense de ce POMDP pourrait elle-même être une construction complexe, dérivée d'une évaluation multi-objectifs des états futurs potentiels (utilisant les techniques du Pilier 3\) et d'un terme explicite de gain d'information (issu des formalismes du Pilier 2). Le tout opérerait sur un état de croyance sur le "Graphe de Potentiel Limité", maintenu et mis à jour via les techniques d'inférence probabiliste du Pilier 4\.

Cette perspective unificatrice suggère que l'architecture globale du PI-QPM pourrait s'inspirer de cadres avancés tels que la planification POMDP hiérarchique ou l'apprentissage par renforcement hiérarchique et multi-objectifs, où les différents composants algorithmiques interagissent de manière structurée et théoriquement fondée. Une telle intégration met également en lumière la formidable complexité, tant computationnelle que théorique, d'un système aussi ambitieux que PI-QPM, mais aussi son potentiel pour repousser les frontières de la gestion de projet agentique.

#### **Sources des citations**

1. Laying the Foundations for Solving FOND HTN Problems ... \- IJCAI, consulté le juin 11, 2025, [https://www.ijcai.org/proceedings/2024/0751.pdf](https://www.ijcai.org/proceedings/2024/0751.pdf)  
2. Workshop | Grav \- Hierarchical Planning, consulté le juin 11, 2025, [https://hierarchical-task.net/hplan](https://hierarchical-task.net/hplan)  
3. An Accurate HDDL Domain Learning Algorithm from Partial and ..., consulté le juin 11, 2025, [https://icaps22.icaps-conference.org/workshops/HPlan/papers/paper-01.pdf](https://icaps22.icaps-conference.org/workshops/HPlan/papers/paper-01.pdf)  
4. Learning Hierarchical Task Knowledge for Planning, consulté le juin 11, 2025, [http://www.isle.org/\~langley/papers/procedure.aaai25.pdf](http://www.isle.org/~langley/papers/procedure.aaai25.pdf)  
5. Learning Hierarchical Task Knowledge for Planning | Proceedings of the AAAI Conference on Artificial Intelligence, consulté le juin 11, 2025, [https://ojs.aaai.org/index.php/AAAI/article/view/35091](https://ojs.aaai.org/index.php/AAAI/article/view/35091)  
6. Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes, consulté le juin 11, 2025, [https://rjournal.github.io/articles/RJ-2024-021/](https://rjournal.github.io/articles/RJ-2024-021/)  
7. Deep reinforcement learning for static noisy state feedback control with reward estimation, consulté le juin 11, 2025, [https://www.tandfonline.com/doi/full/10.1080/01691864.2025.2468215?src=](https://www.tandfonline.com/doi/full/10.1080/01691864.2025.2468215?src)  
8. Online POMDP Planning with Anytime Deterministic Optimality Guarantees \- arXiv, consulté le juin 11, 2025, [https://arxiv.org/html/2310.01791v3](https://arxiv.org/html/2310.01791v3)  
9. Online POMDP Planning with Anytime Deterministic ... \- arXiv, consulté le juin 11, 2025, [https://arxiv.org/pdf/2310.01791](https://arxiv.org/pdf/2310.01791)  
10. Anytime Incremental $\\rho $ POMDP Planning in Continuous Spaces, consulté le juin 11, 2025, [https://arxiv.org/pdf/2502.02549](https://arxiv.org/pdf/2502.02549)  
11. (PDF) Anytime Incremental $\\rho$POMDP Planning in Continuous Spaces \- ResearchGate, consulté le juin 11, 2025, [https://www.researchgate.net/publication/388686663\_Anytime\_Incremental\_rhoPOMDP\_Planning\_in\_Continuous\_Spaces](https://www.researchgate.net/publication/388686663_Anytime_Incremental_rhoPOMDP_Planning_in_Continuous_Spaces)  
12. \[2502.02549\] Anytime Incremental $ρ$POMDP Planning in Continuous Spaces \- arXiv, consulté le juin 11, 2025, [https://arxiv.org/abs/2502.02549](https://arxiv.org/abs/2502.02549)  
13. Deep Reinforcement Learning: A Survey | Request PDF \- ResearchGate, consulté le juin 11, 2025, [https://www.researchgate.net/publication/363941353\_Deep\_Reinforcement\_Learning\_A\_Survey](https://www.researchgate.net/publication/363941353_Deep_Reinforcement_Learning_A_Survey)  
14. Deep Reinforcement Learning Using Optimized Monte Carlo Tree Search in EWN | Request PDF \- ResearchGate, consulté le juin 11, 2025, [https://www.researchgate.net/publication/373473485\_Deep\_Reinforcement\_Learning\_Using\_Optimized\_Monte\_Carlo\_Tree\_Search\_in\_EWN](https://www.researchgate.net/publication/373473485_Deep_Reinforcement_Learning_Using_Optimized_Monte_Carlo_Tree_Search_in_EWN)  
15. arxiv.org, consulté le juin 11, 2025, [https://arxiv.org/html/2406.10667v2](https://arxiv.org/html/2406.10667v2)  
16. UniZero: Generalized and Efficient Planning with Scalable Latent World Models, consulté le juin 11, 2025, [https://openreview.net/forum?id=Gl6dF9soQo](https://openreview.net/forum?id=Gl6dF9soQo)  
17. UniZero: Generalized and Efficient Planning with Scalable Latent ..., consulté le juin 11, 2025, [https://arxiv.org/pdf/2406.10667?](https://arxiv.org/pdf/2406.10667)  
18. ICML 2024 Papers, consulté le juin 11, 2025, [https://icml.cc/virtual/2024/papers.html](https://icml.cc/virtual/2024/papers.html)  
19. arxiv.org, consulté le juin 11, 2025, [https://arxiv.org/html/2505.14656v1](https://arxiv.org/html/2505.14656v1)  
20. Monte Carlo Tree Search for Integrated Planning, Learning, and Execution in Nondeterministic Python, consulté le juin 11, 2025, [https://ntrs.nasa.gov/api/citations/20240004045/downloads/propel\_icaps\_4\_4\_24.pdf](https://ntrs.nasa.gov/api/citations/20240004045/downloads/propel_icaps_4_4_24.pdf)  
21. (PDF) Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning, consulté le juin 11, 2025, [https://www.researchgate.net/publication/391910610\_Cost-Augmented\_Monte\_Carlo\_Tree\_Search\_for\_LLM-Assisted\_Planning](https://www.researchgate.net/publication/391910610_Cost-Augmented_Monte_Carlo_Tree_Search_for_LLM-Assisted_Planning)  
22. active learning and bayesian optimization: a unified perspective to learn with a goal \- arXiv, consulté le juin 11, 2025, [https://arxiv.org/pdf/2303.01560](https://arxiv.org/pdf/2303.01560)  
23. Bayesian Optimization and Active Learning Cookbook \- PhysicsX, consulté le juin 11, 2025, [https://www.physicsx.ai/newsroom/bayesian-optimization-and-active-learning-cookbook](https://www.physicsx.ai/newsroom/bayesian-optimization-and-active-learning-cookbook)  
24. Model-Free Active Exploration in Reinforcement Learning \- NIPS papers, consulté le juin 11, 2025, [https://papers.nips.cc/paper\_files/paper/2023/file/abbbb25cddb2c2cd08714e6bfa2f0634-Supplemental-Conference.pdf](https://papers.nips.cc/paper_files/paper/2023/file/abbbb25cddb2c2cd08714e6bfa2f0634-Supplemental-Conference.pdf)  
25. Model-Free Active Exploration in Reinforcement ... \- NIPS papers, consulté le juin 11, 2025, [https://proceedings.neurips.cc/paper\_files/paper/2023/file/abbbb25cddb2c2cd08714e6bfa2f0634-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/abbbb25cddb2c2cd08714e6bfa2f0634-Paper-Conference.pdf)  
26. MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition \- SciSpace, consulté le juin 11, 2025, [https://scispace.com/pdf/moea-d-a-multiobjective-evolutionary-algorithm-based-on-1ocq44zhaf.pdf](https://scispace.com/pdf/moea-d-a-multiobjective-evolutionary-algorithm-based-on-1ocq44zhaf.pdf)  
27. New search strategy for multi-objective evolutionary algorithm \- PMC, consulté le juin 11, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11667601/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11667601/)  
28. arXiv:2412.07444v1 \[cs.NE\] 10 Dec 2024, consulté le juin 11, 2025, [https://ris.utwente.nl/ws/portalfiles/portal/478160856/2412.07444v1.pdf](https://ris.utwente.nl/ws/portalfiles/portal/478160856/2412.07444v1.pdf)  
29. Multi-Objective Quality-Diversity in Unstructured and Unbounded Spaces \- arXiv, consulté le juin 11, 2025, [https://arxiv.org/html/2504.03715v1](https://arxiv.org/html/2504.03715v1)  
30. \[2504.03715\] Multi-Objective Quality-Diversity in Unstructured and Unbounded Spaces \- arXiv, consulté le juin 11, 2025, [https://arxiv.org/abs/2504.03715](https://arxiv.org/abs/2504.03715)  
31. (PDF) Multi-Objective Quality-Diversity in Unstructured and Unbounded Spaces, consulté le juin 11, 2025, [https://www.researchgate.net/publication/390571035\_Multi-Objective\_Quality-Diversity\_in\_Unstructured\_and\_Unbounded\_Spaces](https://www.researchgate.net/publication/390571035_Multi-Objective_Quality-Diversity_in_Unstructured_and_Unbounded_Spaces)  
32. Hypervolume Indicator \- Worked Through Example \- YouTube, consulté le juin 11, 2025, [https://www.youtube.com/watch?v=5c4hVd1CEAg](https://www.youtube.com/watch?v=5c4hVd1CEAg)  
33. Pareto Prompt Optimization \- OSTI, consulté le juin 11, 2025, [https://www.osti.gov/servlets/purl/2543057](https://www.osti.gov/servlets/purl/2543057)  
34. Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume, consulté le juin 11, 2025, [https://arxiv.org/html/2403.05100v2](https://arxiv.org/html/2403.05100v2)  
35. Thesis Proposal: Uncertainty in Knowledge Graph ... \- ACL Anthology, consulté le juin 11, 2025, [https://aclanthology.org/2025.naacl-srw.4.pdf](https://aclanthology.org/2025.naacl-srw.4.pdf)  
36. Vol. 39 No. 11: AAAI-25 Technical Tracks 11 | Proceedings of the AAAI Conference on Artificial Intelligence, consulté le juin 11, 2025, [https://ojs.aaai.org/index.php/AAAI/issue/view/634](https://ojs.aaai.org/index.php/AAAI/issue/view/634)  
37. Probabilistic soft logic \- Wikipedia, consulté le juin 11, 2025, [https://en.wikipedia.org/wiki/Probabilistic\_soft\_logic](https://en.wikipedia.org/wiki/Probabilistic_soft_logic)  
38. Markov Logic Networks \- University of Washington, consulté le juin 11, 2025, [https://homes.cs.washington.edu/\~pedrod/kbmn.pdf](https://homes.cs.washington.edu/~pedrod/kbmn.pdf)  
39. Belief propagation \- Wikipedia, consulté le juin 11, 2025, [https://en.wikipedia.org/wiki/Belief\_propagation](https://en.wikipedia.org/wiki/Belief_propagation)  
40. On Loopy Belief Propagation for SASCAs \- IACR Communications in Cryptology, consulté le juin 11, 2025, [https://cic.iacr.org/p/1/4/15/pdf](https://cic.iacr.org/p/1/4/15/pdf)  
41. Message Passing Dynamics of Belief Propagation Algorithms \- Brown Computer Science, consulté le juin 11, 2025, [https://cs.brown.edu/people/pfelzens/dissertations/grim-phd-dissertation.pdf](https://cs.brown.edu/people/pfelzens/dissertations/grim-phd-dissertation.pdf)  
42. On Loopy Belief Propagation for SASCAs \- IACR Communications in Cryptology, consulté le juin 11, 2025, [https://cic.iacr.org/p/1/4/15](https://cic.iacr.org/p/1/4/15)  
43. Generalized Belief Propagation, consulté le juin 11, 2025, [https://proceedings.neurips.cc/paper/1832-generalized-belief-propagation.pdf](https://proceedings.neurips.cc/paper/1832-generalized-belief-propagation.pdf)  
44. Adaptive Variational Inference in Probabilistic Graphical Models: Beyond Bethe, Tree-Reweighted, and Convex Free Energies \- arXiv, consulté le juin 11, 2025, [https://arxiv.org/html/2502.03341v1](https://arxiv.org/html/2502.03341v1)  
45. \[2502.03341\] Adaptive Variational Inference in Probabilistic Graphical Models: Beyond Bethe, Tree-Reweighted, and Convex Free Energies \- arXiv, consulté le juin 11, 2025, [https://arxiv.org/abs/2502.03341](https://arxiv.org/abs/2502.03341)  
46. Partially factorized variational inference for high-dimensional mixed models | Biometrika | Oxford Academic, consulté le juin 11, 2025, [https://academic.oup.com/biomet/article/112/2/asae067/7914011](https://academic.oup.com/biomet/article/112/2/asae067/7914011)  
47. Adaptive Sampling using POMDPs with Domain-Specific Considerations \- USC Dornsife, consulté le juin 11, 2025, [https://dornsife.usc.edu/caron/wp-content/uploads/sites/263/2023/11/2021\_Salhotra\_etal\_Robotics-Adaptive-Sampling.pdf](https://dornsife.usc.edu/caron/wp-content/uploads/sites/263/2023/11/2021_Salhotra_etal_Robotics-Adaptive-Sampling.pdf)  
48. Scaling Long-Horizon Online POMDP Planning via Rapid State Space Sampling \- arXiv, consulté le juin 11, 2025, [https://arxiv.org/html/2411.07032v1](https://arxiv.org/html/2411.07032v1)